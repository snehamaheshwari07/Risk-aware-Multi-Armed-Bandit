{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = whole set \n",
    "#df = set of selected assets only\n",
    "#return = log(c_{i,t}/c_{i,t+1}\n",
    "#reward = (4/3)*(1+((np.pi)*(np.arctan(np.log(df.loc[:,'H'+str(t)])))))**2-(4/3)\n",
    "#T[i]= number of times asset i is selected for set S (started it from 20)\n",
    "#mu= algoritm confidence level belongs to (0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions :\n",
    "#  get_cost(t)  \n",
    "#estimated_reward\n",
    "#estimated_reward_plus\n",
    "#estimated_reward_minus\n",
    "#get_cost_for_k\n",
    "#get_start_point() it gives x_{i,1}'s for all i's\n",
    "#negative_cvar\n",
    "#et_weight_from_securities() #get weights of each asset from x_{i,t}'s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to generate a random name of stock\n",
    "def id_generator(size=6, chars=string.ascii_uppercase + string.digits):\n",
    "    return ''.join(random.choice(chars) for _ in range(size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to generate random cost at 1st trial of stock\n",
    "def cost_generator(cost_low, cost_up):\n",
    "    return random.uniform(cost_low,cost_up)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing data \n",
    "dataset=pd.read_csv(r'C:\\Users\\Sneha\\Desktop\\dataa.csv')\n",
    "#dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#theta\n",
    "theta=5\n",
    "#Cost Bound\n",
    "Cost_Bound=100\n",
    "#number of assets\n",
    "no_of_assets=100\n",
    "#algorithm satisfy constraints with probability atleast (1-mu) \n",
    "mu= 0.1\n",
    "#number of R_{i,t} trials\n",
    "no_of_trials=2000\n",
    "\n",
    "#appending rows to dataset\n",
    "p=dataset\n",
    "for j in range(0,no_of_assets):\n",
    "    p=p.append({'Name':id_generator(),'Cost_1':cost_generator(5,15),'H1':cost_generator(0,1)},ignore_index=True)\n",
    "\n",
    "dataset=p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating columns\n",
    "column=[]\n",
    "for j in range(0,2019):\n",
    "    x= np.random.uniform(low=0, high=2*dataset.loc[:,'H1'], size=no_of_assets)\n",
    "    column.append(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#naming the headers for historical data rewards\n",
    "h=[]\n",
    "for j in range(2,21):\n",
    "    h.append('H'+str(j))\n",
    "\n",
    "#naming the headers for current data rewards\n",
    "r=[]\n",
    "for j in range(1,no_of_trials+1):\n",
    "    r.append('R'+str(j))\n",
    "#appending columns \n",
    "for j in range (0,19):\n",
    "    dataset[h[j]]=column[j]\n",
    "for j in range(19,no_of_trials+19):\n",
    "    dataset[r[j-19]]=column[j]\n",
    "\n",
    "\n",
    "#dataset\n",
    "# here columns of dataset represent (c_{i,t+1}/c_{i,t}) \n",
    "#considering trial 1 at Cost_1\n",
    "# H1 implies c_{i,2}/c_{i,1}\n",
    "#cost at trial 2 is H1*Cost_1\n",
    "#at trial 2 H2 is unknown \n",
    "#reward at trial 2 is determined using H2 \n",
    "#if H20 is considered to be the start\n",
    "#then cost at trial 1 is multiplication of columns till H20(all historical returns)\n",
    "#trial 1 begins from R1, where cost will be calculated by multiplying dataset entries till 20\n",
    "# And trial 1 estimated reward vector will be mean of reward till H20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cost(t):\n",
    "    if (t==1):\n",
    "        y=np.multiply(dataset.loc[:,\"Cost_1\"],dataset.loc[:,\"H1\"])\n",
    "        i=2\n",
    "        while (i<=20):\n",
    "            y=np.multiply(y,dataset.loc[:,\"H\"+str(i)])\n",
    "            i=i+1\n",
    "    else:\n",
    "        y=np.multiply(dataset.loc[:,\"Cost_1\"],dataset.loc[:,\"H1\"])\n",
    "        i=2\n",
    "        while (i<=20):\n",
    "            y=np.multiply(y,dataset.loc[:,\"H\"+str(i)])\n",
    "            i=i+1\n",
    "        j=1\n",
    "        while (j<=(t-1)):\n",
    "            y=np.multiply(y,dataset.loc[:,\"R\"+str(j)])\n",
    "            j=j+1   \n",
    "            \n",
    "    return list(y)\n",
    "#get_cost(2) will return cost of assets at trial 2 by multiplying till R1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing of cost vector \n",
    "#y=np.multiply(dataset.loc[:,\"Cost_1\"],dataset.loc[:,\"H1\"])\n",
    "#i=2\n",
    "#while (i<=20):\n",
    "#    y=np.multiply(y,dataset.loc[:,\"H\"+str(i)])\n",
    "#    i=i+1\n",
    "#y\n",
    "#np.multiply(y, dataset.loc[:,\"R1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing np.multiply\n",
    "#a=[1,2]\n",
    "#b=[2,3]\n",
    "#c=[3,4]\n",
    "#np.multiply(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge \tWeight\n",
      "6 - 1 \t 0.7098567805693193\n",
      "4 - 2 \t 0.41336633138762985\n",
      "90 - 3 \t 0.44355946652013445\n",
      "18 - 4 \t 0.2644541849977907\n",
      "9 - 5 \t 0.432812943287486\n",
      "85 - 6 \t 0.48923503699219173\n",
      "18 - 7 \t 0.21533545498345877\n",
      "99 - 8 \t 0.894354607514534\n",
      "18 - 9 \t 0.2602714078238845\n",
      "56 - 10 \t 0.772311655429493\n",
      "69 - 11 \t 0.6534870476262067\n",
      "70 - 12 \t 1.0982324190591501\n",
      "38 - 13 \t 0.556383132527678\n",
      "43 - 14 \t 0.7781394715054556\n",
      "32 - 15 \t 0.4285844490986843\n",
      "9 - 16 \t 0.48880131202853266\n",
      "0 - 17 \t 0.37709014780444555\n",
      "17 - 18 \t 0.30820153843939857\n",
      "85 - 19 \t 0.69892884668353\n",
      "31 - 20 \t 0.8450535576610966\n",
      "16 - 21 \t 0.8292376169978634\n",
      "18 - 22 \t 0.23357432309145823\n",
      "9 - 23 \t 0.28001636968279825\n",
      "89 - 24 \t 0.517549487627788\n",
      "50 - 25 \t 0.5225392918697314\n",
      "16 - 26 \t 0.5525219708660203\n",
      "35 - 27 \t 0.6468320624144827\n",
      "82 - 28 \t 0.5514418549672532\n",
      "22 - 29 \t 0.42962270198073615\n",
      "18 - 30 \t 0.23623301888075654\n",
      "88 - 31 \t 0.48488027935653566\n",
      "30 - 32 \t 0.4168852908907883\n",
      "22 - 33 \t 0.45307002728124707\n",
      "32 - 34 \t 0.43695842061819135\n",
      "38 - 35 \t 0.4510716093755412\n",
      "89 - 36 \t 0.7442543061572143\n",
      "4 - 37 \t 0.8083427808818862\n",
      "18 - 38 \t 0.3922843688137627\n",
      "75 - 39 \t 0.7861751808437438\n",
      "83 - 40 \t 0.7007984588173247\n",
      "48 - 41 \t 1.0428373966494084\n",
      "93 - 42 \t 0.38302520995559053\n",
      "7 - 43 \t 0.4246353667615325\n",
      "15 - 44 \t 0.797123011704558\n",
      "48 - 45 \t 0.8623506469512324\n",
      "3 - 46 \t 0.7720723763329056\n",
      "6 - 47 \t 0.4821198996778355\n",
      "99 - 48 \t 0.7811087021297073\n",
      "23 - 49 \t 0.49632778749745404\n",
      "99 - 50 \t 0.715292646374564\n",
      "50 - 51 \t 0.7018070119004586\n",
      "71 - 52 \t 0.8319500717807572\n",
      "33 - 53 \t 0.5195611840578878\n",
      "65 - 54 \t 0.83906036214857\n",
      "90 - 55 \t 0.5014873804032097\n",
      "0 - 56 \t 0.6201350472458133\n",
      "2 - 57 \t 0.44594051769978166\n",
      "86 - 58 \t 0.8872304101958713\n",
      "7 - 59 \t 0.3327427706906586\n",
      "4 - 60 \t 0.695126997348537\n",
      "88 - 61 \t 0.3638308604645663\n",
      "16 - 62 \t 0.5721317718816573\n",
      "53 - 63 \t 0.5183303338434339\n",
      "50 - 64 \t 0.9836913120242549\n",
      "88 - 65 \t 0.42306426047429313\n",
      "28 - 66 \t 0.7466103724862933\n",
      "9 - 67 \t 0.7063061476068512\n",
      "95 - 68 \t 0.46859827033635904\n",
      "4 - 69 \t 0.6918160681164991\n",
      "29 - 70 \t 0.8716945872331942\n",
      "90 - 71 \t 0.626338357867797\n",
      "26 - 72 \t 1.0072266425230183\n",
      "90 - 73 \t 0.5165907294449233\n",
      "22 - 74 \t 0.49693998721458976\n",
      "99 - 75 \t 0.7524134052674877\n",
      "47 - 76 \t 0.7441395399007518\n",
      "49 - 77 \t 0.8793702381519329\n",
      "27 - 78 \t 0.7440120850298694\n",
      "51 - 79 \t 0.8395577495591589\n",
      "76 - 80 \t 0.8195744378262904\n",
      "91 - 81 \t 0.9042903050853234\n",
      "7 - 82 \t 0.38941513824917773\n",
      "90 - 83 \t 0.48695838861020346\n",
      "44 - 84 \t 0.9934295170740693\n",
      "30 - 85 \t 0.44246543559427765\n",
      "93 - 86 \t 0.7957691140115534\n",
      "24 - 87 \t 0.8566098751400819\n",
      "7 - 88 \t 0.22752277879788219\n",
      "32 - 89 \t 0.5772464166606385\n",
      "93 - 90 \t 0.419461273089391\n",
      "47 - 91 \t 0.8213387221653295\n",
      "60 - 92 \t 0.7295599758380726\n",
      "18 - 93 \t 0.19433606119697755\n",
      "38 - 94 \t 0.4540772754333757\n",
      "98 - 95 \t 0.4321249277180973\n",
      "38 - 96 \t 0.39701309420216524\n",
      "71 - 97 \t 0.9079635493263863\n",
      "18 - 98 \t 0.4532833548355631\n",
      "5 - 99 \t 0.6277651243156889\n",
      "[0, 7, 5, 91, 19, 10, 86, 19, 100, 19, 57, 70, 71, 39, 44, 33, 10, 1, 18, 86, 32, 17, 19, 10, 90, 51, 17, 36, 83, 23, 19, 89, 31, 23, 33, 39, 90, 5, 19, 76, 84, 49, 94, 8, 16, 49, 4, 7, 100, 24, 100, 51, 72, 34, 66, 91, 1, 3, 87, 8, 5, 89, 17, 54, 51, 89, 29, 10, 96, 5, 30, 91, 27, 91, 23, 100, 48, 50, 28, 52, 77, 92, 8, 91, 45, 31, 94, 25, 8, 33, 94, 48, 61, 19, 39, 99, 39, 72, 19, 6]\n"
     ]
    }
   ],
   "source": [
    "#construction of complete graph and then min spanning tree to choose a set of assets and used prims algo to construct min spanning tree\n",
    "\n",
    "\n",
    "from math import log\n",
    "\n",
    "#historical return vectors (return implies log(c_{i,t}/c_{i,t+1}))\n",
    "historical_returns=[]\n",
    "for i in range(0,no_of_assets):\n",
    "    ro=dataset.iloc[i]\n",
    "    \n",
    "    row=ro[2:22]#here in 1:5 1 is included and 5 is not included \n",
    "    k=[log(y) for y in row]\n",
    "    historical_returns.append(k)\n",
    "\n",
    "#historical_returns[0]\n",
    "\n",
    "#-----------------------------------------\n",
    "delta=[]\n",
    "edge_length=[]\n",
    "for i in range(0,no_of_assets):\n",
    "    delta.append([])\n",
    "    for j in range(0,no_of_assets):\n",
    "        p=[]#h_{i,t}*h_{j,t}\n",
    "        for t in range(0,20):\n",
    "            p.append(historical_returns[i][t]*historical_returns[j][t])\n",
    "        q=[]#h_{i,t}\n",
    "        for t in range(0,20):\n",
    "            q.append(historical_returns[i][t])\n",
    "        o=[]#h_{j,t}\n",
    "        for t in range(0,20):\n",
    "            o.append(historical_returns[j][t])\n",
    "        s=[]#h_{i,t}^2\n",
    "        for t in range(0,20):\n",
    "            s.append(historical_returns[i][t]*historical_returns[i][t])\n",
    "        m=[]#h_{j,t}^2\n",
    "        for t in range(0,20):\n",
    "            m.append(historical_returns[j][t]*historical_returns[j][t])\n",
    "\n",
    "        delta[i].append(((no_of_assets*sum(p))-(sum(q)*sum(o)))/np.sqrt(((no_of_assets*sum(s))-(sum(q)*sum(q)))*((no_of_assets*sum(m))-(sum(o)*sum(o)))))\n",
    "        \n",
    "for i in range(0,no_of_assets):\n",
    "    edge_length.append([])\n",
    "    for j in range(0,no_of_assets):\n",
    "        y=np.sqrt(2*(1-delta[i][j]))\n",
    "        edge_length[i].append(y)\n",
    "#edge_length\n",
    "\n",
    "#--------------------------------------------\n",
    "#prim's algo to calculate adjecency matrix for minimum spanning tree\n",
    "import sys # Library for INT_MAX \n",
    "from itertools import chain\n",
    "from operator import sub\n",
    "imap=map\n",
    "  \n",
    "class Graph(): \n",
    "  \n",
    "    def __init__(self, vertices): \n",
    "        self.V = vertices \n",
    "        self.graph = [[0 for column in range(vertices)]  \n",
    "                    for row in range(vertices)] \n",
    "  \n",
    "    # A utility function to print the constructed MST stored in parent[] \n",
    "    def printMST(self, parent): \n",
    "        print(\"Edge \\tWeight\")\n",
    "        for i in range(1,self.V):\n",
    "            print(parent[i],\"-\",i,\"\\t\",self.graph[i][ parent[i] ])\n",
    "            \n",
    "\n",
    "    \n",
    "    def missing_numbers(self,parent):\n",
    "        parent.sort()\n",
    "        original_list = [x for x in range(parent[0], no_of_assets + 1)]\n",
    "        parent = set(parent)\n",
    "        return (list(parent ^ set(original_list)))\n",
    "    \n",
    "  \n",
    "    # A utility function to find the vertex with  \n",
    "    # minimum distance value, from the set of vertices  \n",
    "    # not yet included in shortest path tree \n",
    "    def minKey(self, key, mstSet): \n",
    "  \n",
    "        # Initilaize min value \n",
    "        min = sys.maxsize \n",
    "  \n",
    "        for v in range(self.V): \n",
    "            if key[v] < min and mstSet[v] == False: \n",
    "                min = key[v] \n",
    "                min_index = v \n",
    "  \n",
    "        return min_index \n",
    "  \n",
    "    # Function to construct and print MST for a graph  \n",
    "    # represented using adjacency matrix representation \n",
    "    def primMST(self): \n",
    "  \n",
    "        #Key values used to pick minimum weight edge in cut \n",
    "        key = [sys.maxsize] * self.V \n",
    "        parent = [None] * self.V # Array to store constructed MST \n",
    "        # Make key 0 so that this vertex is picked as first vertex \n",
    "        key[0] = 0 \n",
    "        mstSet = [False] * self.V \n",
    "  \n",
    "        parent[0] = -1 # First node is always the root of \n",
    "  \n",
    "        for cout in range(self.V): \n",
    "  \n",
    "            # Pick the minimum distance vertex from  \n",
    "            # the set of vertices not yet processed.  \n",
    "            # u is always equal to src in first iteration \n",
    "            u = self.minKey(key, mstSet) \n",
    "  \n",
    "            # Put the minimum distance vertex in  \n",
    "            # the shortest path tree \n",
    "            mstSet[u] = True\n",
    "  \n",
    "            # Update dist value of the adjacent vertices  \n",
    "            # of the picked vertex only if the current  \n",
    "            # distance is greater than new distance and \n",
    "            # the vertex in not in the shotest path tree \n",
    "            for v in range(self.V): \n",
    "                # graph[u][v] is non zero only for adjacent vertices of m \n",
    "                # mstSet[v] is false for vertices not yet included in MST \n",
    "                # Update the key only if graph[u][v] is smaller than key[v] \n",
    "                if self.graph[u][v] > 0 and mstSet[v] == False and key[v] > self.graph[u][v]: \n",
    "                        key[v] = self.graph[u][v] \n",
    "                        parent[v] = u \n",
    "  \n",
    "        self.printMST(parent)\n",
    "        new_list = [x+1 for x in parent]\n",
    "        print(new_list)\n",
    "        return(self.missing_numbers(new_list))\n",
    "        \n",
    "\n",
    "g = Graph(no_of_assets) \n",
    "n = no_of_assets\n",
    "matrix = np.zeros((n,no_of_assets)) # Pre-allocate matrix\n",
    "for i in range(0,n):\n",
    "    matrix[i,:] = edge_length[i]\n",
    "g.graph = matrix\n",
    "  \n",
    "k=g.primMST(); \n",
    "#k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Cost_1</th>\n",
       "      <th>H1</th>\n",
       "      <th>H2</th>\n",
       "      <th>H3</th>\n",
       "      <th>H4</th>\n",
       "      <th>H5</th>\n",
       "      <th>H6</th>\n",
       "      <th>H7</th>\n",
       "      <th>H8</th>\n",
       "      <th>...</th>\n",
       "      <th>R1991</th>\n",
       "      <th>R1992</th>\n",
       "      <th>R1993</th>\n",
       "      <th>R1994</th>\n",
       "      <th>R1995</th>\n",
       "      <th>R1996</th>\n",
       "      <th>R1997</th>\n",
       "      <th>R1998</th>\n",
       "      <th>R1999</th>\n",
       "      <th>R2000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PV2IPX</td>\n",
       "      <td>5.038117</td>\n",
       "      <td>0.835735</td>\n",
       "      <td>1.589758</td>\n",
       "      <td>1.487095</td>\n",
       "      <td>0.983900</td>\n",
       "      <td>0.248905</td>\n",
       "      <td>0.038034</td>\n",
       "      <td>1.520773</td>\n",
       "      <td>0.565814</td>\n",
       "      <td>...</td>\n",
       "      <td>0.374233</td>\n",
       "      <td>0.079713</td>\n",
       "      <td>0.331840</td>\n",
       "      <td>0.445684</td>\n",
       "      <td>1.014169</td>\n",
       "      <td>0.446604</td>\n",
       "      <td>1.307291</td>\n",
       "      <td>0.501331</td>\n",
       "      <td>1.395232</td>\n",
       "      <td>0.567960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>T7DTIG</td>\n",
       "      <td>6.170093</td>\n",
       "      <td>0.861015</td>\n",
       "      <td>1.491885</td>\n",
       "      <td>0.388704</td>\n",
       "      <td>0.303404</td>\n",
       "      <td>0.291861</td>\n",
       "      <td>1.185245</td>\n",
       "      <td>1.110719</td>\n",
       "      <td>0.401375</td>\n",
       "      <td>...</td>\n",
       "      <td>1.363263</td>\n",
       "      <td>0.005645</td>\n",
       "      <td>0.707717</td>\n",
       "      <td>1.441140</td>\n",
       "      <td>0.230907</td>\n",
       "      <td>1.123617</td>\n",
       "      <td>0.363932</td>\n",
       "      <td>1.323259</td>\n",
       "      <td>1.092587</td>\n",
       "      <td>1.146443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HTEDMB</td>\n",
       "      <td>5.337605</td>\n",
       "      <td>0.601614</td>\n",
       "      <td>0.717822</td>\n",
       "      <td>1.158810</td>\n",
       "      <td>0.699972</td>\n",
       "      <td>0.406064</td>\n",
       "      <td>0.765946</td>\n",
       "      <td>0.846085</td>\n",
       "      <td>0.735594</td>\n",
       "      <td>...</td>\n",
       "      <td>0.466795</td>\n",
       "      <td>0.266191</td>\n",
       "      <td>0.717479</td>\n",
       "      <td>0.150243</td>\n",
       "      <td>1.041188</td>\n",
       "      <td>0.882118</td>\n",
       "      <td>1.036286</td>\n",
       "      <td>0.145335</td>\n",
       "      <td>0.402870</td>\n",
       "      <td>0.185906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CEPLFO</td>\n",
       "      <td>10.284382</td>\n",
       "      <td>0.671644</td>\n",
       "      <td>1.084973</td>\n",
       "      <td>1.168672</td>\n",
       "      <td>0.724060</td>\n",
       "      <td>0.310944</td>\n",
       "      <td>0.257220</td>\n",
       "      <td>1.009307</td>\n",
       "      <td>1.291811</td>\n",
       "      <td>...</td>\n",
       "      <td>0.542760</td>\n",
       "      <td>1.312476</td>\n",
       "      <td>0.264253</td>\n",
       "      <td>1.048437</td>\n",
       "      <td>1.063177</td>\n",
       "      <td>0.150586</td>\n",
       "      <td>0.940793</td>\n",
       "      <td>0.710916</td>\n",
       "      <td>0.823709</td>\n",
       "      <td>0.440530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CNBYS3</td>\n",
       "      <td>9.356291</td>\n",
       "      <td>0.937390</td>\n",
       "      <td>0.300731</td>\n",
       "      <td>1.632274</td>\n",
       "      <td>0.908328</td>\n",
       "      <td>1.715092</td>\n",
       "      <td>1.622313</td>\n",
       "      <td>1.394172</td>\n",
       "      <td>1.437609</td>\n",
       "      <td>...</td>\n",
       "      <td>1.632324</td>\n",
       "      <td>0.159648</td>\n",
       "      <td>0.915547</td>\n",
       "      <td>0.612437</td>\n",
       "      <td>1.412655</td>\n",
       "      <td>1.162172</td>\n",
       "      <td>0.812511</td>\n",
       "      <td>1.351329</td>\n",
       "      <td>0.532182</td>\n",
       "      <td>0.756806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2XZT21</td>\n",
       "      <td>13.258820</td>\n",
       "      <td>0.326078</td>\n",
       "      <td>0.164465</td>\n",
       "      <td>0.513230</td>\n",
       "      <td>0.422354</td>\n",
       "      <td>0.345787</td>\n",
       "      <td>0.530717</td>\n",
       "      <td>0.006348</td>\n",
       "      <td>0.035507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.496231</td>\n",
       "      <td>0.090437</td>\n",
       "      <td>0.623189</td>\n",
       "      <td>0.599199</td>\n",
       "      <td>0.246096</td>\n",
       "      <td>0.592078</td>\n",
       "      <td>0.512876</td>\n",
       "      <td>0.089486</td>\n",
       "      <td>0.282901</td>\n",
       "      <td>0.463123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>DNN3DP</td>\n",
       "      <td>10.093455</td>\n",
       "      <td>0.876985</td>\n",
       "      <td>0.088443</td>\n",
       "      <td>0.383761</td>\n",
       "      <td>0.305644</td>\n",
       "      <td>0.355502</td>\n",
       "      <td>1.045151</td>\n",
       "      <td>0.413347</td>\n",
       "      <td>0.469768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.371582</td>\n",
       "      <td>0.134904</td>\n",
       "      <td>0.560739</td>\n",
       "      <td>1.476387</td>\n",
       "      <td>0.303390</td>\n",
       "      <td>0.390836</td>\n",
       "      <td>0.348461</td>\n",
       "      <td>1.670531</td>\n",
       "      <td>0.517499</td>\n",
       "      <td>1.505587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0V3A2X</td>\n",
       "      <td>14.814455</td>\n",
       "      <td>0.610612</td>\n",
       "      <td>0.191412</td>\n",
       "      <td>0.010825</td>\n",
       "      <td>0.089612</td>\n",
       "      <td>0.223186</td>\n",
       "      <td>0.046246</td>\n",
       "      <td>0.350807</td>\n",
       "      <td>0.338857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.427678</td>\n",
       "      <td>0.458192</td>\n",
       "      <td>0.109410</td>\n",
       "      <td>0.658182</td>\n",
       "      <td>0.767887</td>\n",
       "      <td>0.256473</td>\n",
       "      <td>1.043085</td>\n",
       "      <td>0.068918</td>\n",
       "      <td>0.523834</td>\n",
       "      <td>0.979901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1NIXN8</td>\n",
       "      <td>13.551261</td>\n",
       "      <td>0.968877</td>\n",
       "      <td>0.216392</td>\n",
       "      <td>0.504142</td>\n",
       "      <td>0.289065</td>\n",
       "      <td>0.575558</td>\n",
       "      <td>1.225706</td>\n",
       "      <td>1.832520</td>\n",
       "      <td>0.696728</td>\n",
       "      <td>...</td>\n",
       "      <td>0.210890</td>\n",
       "      <td>0.744018</td>\n",
       "      <td>0.974609</td>\n",
       "      <td>0.960943</td>\n",
       "      <td>1.780807</td>\n",
       "      <td>1.288110</td>\n",
       "      <td>0.717676</td>\n",
       "      <td>1.378187</td>\n",
       "      <td>1.637874</td>\n",
       "      <td>0.619832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>QW3USH</td>\n",
       "      <td>11.800536</td>\n",
       "      <td>0.831968</td>\n",
       "      <td>1.287700</td>\n",
       "      <td>0.728342</td>\n",
       "      <td>0.335391</td>\n",
       "      <td>1.435803</td>\n",
       "      <td>0.851525</td>\n",
       "      <td>0.201946</td>\n",
       "      <td>0.342744</td>\n",
       "      <td>...</td>\n",
       "      <td>1.148539</td>\n",
       "      <td>1.456334</td>\n",
       "      <td>0.464729</td>\n",
       "      <td>1.087713</td>\n",
       "      <td>0.007711</td>\n",
       "      <td>0.364899</td>\n",
       "      <td>0.705076</td>\n",
       "      <td>1.505881</td>\n",
       "      <td>0.625855</td>\n",
       "      <td>0.510318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4KSVFB</td>\n",
       "      <td>8.973407</td>\n",
       "      <td>0.652648</td>\n",
       "      <td>0.967786</td>\n",
       "      <td>0.749000</td>\n",
       "      <td>0.785042</td>\n",
       "      <td>0.011473</td>\n",
       "      <td>1.186318</td>\n",
       "      <td>0.290621</td>\n",
       "      <td>1.133813</td>\n",
       "      <td>...</td>\n",
       "      <td>1.256736</td>\n",
       "      <td>1.146073</td>\n",
       "      <td>0.990398</td>\n",
       "      <td>0.204778</td>\n",
       "      <td>1.025945</td>\n",
       "      <td>1.137440</td>\n",
       "      <td>0.898531</td>\n",
       "      <td>0.080849</td>\n",
       "      <td>1.101648</td>\n",
       "      <td>1.163442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>USIV7B</td>\n",
       "      <td>10.284000</td>\n",
       "      <td>0.193028</td>\n",
       "      <td>0.131014</td>\n",
       "      <td>0.141899</td>\n",
       "      <td>0.144848</td>\n",
       "      <td>0.318133</td>\n",
       "      <td>0.256971</td>\n",
       "      <td>0.277086</td>\n",
       "      <td>0.029861</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142844</td>\n",
       "      <td>0.050576</td>\n",
       "      <td>0.221908</td>\n",
       "      <td>0.258092</td>\n",
       "      <td>0.163441</td>\n",
       "      <td>0.264920</td>\n",
       "      <td>0.072136</td>\n",
       "      <td>0.373083</td>\n",
       "      <td>0.185653</td>\n",
       "      <td>0.061511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2NOPEY</td>\n",
       "      <td>12.706412</td>\n",
       "      <td>0.807162</td>\n",
       "      <td>1.156811</td>\n",
       "      <td>1.404877</td>\n",
       "      <td>0.249776</td>\n",
       "      <td>0.772932</td>\n",
       "      <td>0.996842</td>\n",
       "      <td>0.714230</td>\n",
       "      <td>1.231330</td>\n",
       "      <td>...</td>\n",
       "      <td>1.120322</td>\n",
       "      <td>0.248247</td>\n",
       "      <td>0.130342</td>\n",
       "      <td>0.385066</td>\n",
       "      <td>0.242253</td>\n",
       "      <td>0.868283</td>\n",
       "      <td>0.003301</td>\n",
       "      <td>0.060758</td>\n",
       "      <td>1.122132</td>\n",
       "      <td>0.462768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>EY2ZKX</td>\n",
       "      <td>5.970391</td>\n",
       "      <td>0.845507</td>\n",
       "      <td>0.299383</td>\n",
       "      <td>0.188357</td>\n",
       "      <td>0.802668</td>\n",
       "      <td>0.446692</td>\n",
       "      <td>0.162821</td>\n",
       "      <td>0.471423</td>\n",
       "      <td>0.701029</td>\n",
       "      <td>...</td>\n",
       "      <td>1.250464</td>\n",
       "      <td>1.016952</td>\n",
       "      <td>1.493628</td>\n",
       "      <td>1.644986</td>\n",
       "      <td>0.348333</td>\n",
       "      <td>1.295400</td>\n",
       "      <td>0.243640</td>\n",
       "      <td>1.393367</td>\n",
       "      <td>0.365204</td>\n",
       "      <td>1.356741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>60EDZR</td>\n",
       "      <td>8.214620</td>\n",
       "      <td>0.869059</td>\n",
       "      <td>0.333606</td>\n",
       "      <td>0.292312</td>\n",
       "      <td>1.052218</td>\n",
       "      <td>0.560927</td>\n",
       "      <td>0.832845</td>\n",
       "      <td>1.423188</td>\n",
       "      <td>0.374126</td>\n",
       "      <td>...</td>\n",
       "      <td>1.605534</td>\n",
       "      <td>1.226621</td>\n",
       "      <td>1.662906</td>\n",
       "      <td>0.665913</td>\n",
       "      <td>0.968328</td>\n",
       "      <td>1.036657</td>\n",
       "      <td>0.248921</td>\n",
       "      <td>1.478777</td>\n",
       "      <td>0.994675</td>\n",
       "      <td>1.269654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>SPI66W</td>\n",
       "      <td>6.972012</td>\n",
       "      <td>0.737508</td>\n",
       "      <td>1.344694</td>\n",
       "      <td>1.361780</td>\n",
       "      <td>0.134995</td>\n",
       "      <td>0.585304</td>\n",
       "      <td>0.675788</td>\n",
       "      <td>1.279265</td>\n",
       "      <td>0.945051</td>\n",
       "      <td>...</td>\n",
       "      <td>0.477370</td>\n",
       "      <td>0.765020</td>\n",
       "      <td>0.838403</td>\n",
       "      <td>0.225098</td>\n",
       "      <td>1.049114</td>\n",
       "      <td>0.107519</td>\n",
       "      <td>1.225292</td>\n",
       "      <td>0.114809</td>\n",
       "      <td>0.740481</td>\n",
       "      <td>0.351209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>IO96OP</td>\n",
       "      <td>12.323025</td>\n",
       "      <td>0.786681</td>\n",
       "      <td>0.711246</td>\n",
       "      <td>1.509508</td>\n",
       "      <td>1.320507</td>\n",
       "      <td>1.243221</td>\n",
       "      <td>1.184888</td>\n",
       "      <td>1.404887</td>\n",
       "      <td>1.151514</td>\n",
       "      <td>...</td>\n",
       "      <td>0.541593</td>\n",
       "      <td>0.442798</td>\n",
       "      <td>1.532835</td>\n",
       "      <td>1.118471</td>\n",
       "      <td>0.230931</td>\n",
       "      <td>1.558875</td>\n",
       "      <td>1.123411</td>\n",
       "      <td>1.525556</td>\n",
       "      <td>1.443788</td>\n",
       "      <td>1.440434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>EBWBVY</td>\n",
       "      <td>12.572702</td>\n",
       "      <td>0.114672</td>\n",
       "      <td>0.123311</td>\n",
       "      <td>0.152474</td>\n",
       "      <td>0.138077</td>\n",
       "      <td>0.029465</td>\n",
       "      <td>0.020562</td>\n",
       "      <td>0.206288</td>\n",
       "      <td>0.144936</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099471</td>\n",
       "      <td>0.018669</td>\n",
       "      <td>0.066881</td>\n",
       "      <td>0.181942</td>\n",
       "      <td>0.197919</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>0.021245</td>\n",
       "      <td>0.062013</td>\n",
       "      <td>0.198201</td>\n",
       "      <td>0.002924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0XR98F</td>\n",
       "      <td>6.586332</td>\n",
       "      <td>0.919946</td>\n",
       "      <td>1.778058</td>\n",
       "      <td>0.390447</td>\n",
       "      <td>0.278292</td>\n",
       "      <td>1.342161</td>\n",
       "      <td>0.923665</td>\n",
       "      <td>0.771908</td>\n",
       "      <td>0.795025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.860271</td>\n",
       "      <td>0.216450</td>\n",
       "      <td>0.436777</td>\n",
       "      <td>1.804138</td>\n",
       "      <td>1.169294</td>\n",
       "      <td>1.742070</td>\n",
       "      <td>1.100735</td>\n",
       "      <td>0.986341</td>\n",
       "      <td>0.623001</td>\n",
       "      <td>0.052091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>3OW6RA</td>\n",
       "      <td>10.136208</td>\n",
       "      <td>0.806013</td>\n",
       "      <td>0.403378</td>\n",
       "      <td>0.138196</td>\n",
       "      <td>1.377880</td>\n",
       "      <td>0.709778</td>\n",
       "      <td>1.301230</td>\n",
       "      <td>0.925927</td>\n",
       "      <td>0.374428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051531</td>\n",
       "      <td>1.023437</td>\n",
       "      <td>1.022839</td>\n",
       "      <td>0.001232</td>\n",
       "      <td>1.518495</td>\n",
       "      <td>1.193740</td>\n",
       "      <td>1.402546</td>\n",
       "      <td>1.096816</td>\n",
       "      <td>0.203422</td>\n",
       "      <td>0.364915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>XFWIU4</td>\n",
       "      <td>10.701527</td>\n",
       "      <td>0.816055</td>\n",
       "      <td>0.382129</td>\n",
       "      <td>0.353656</td>\n",
       "      <td>0.250442</td>\n",
       "      <td>0.390416</td>\n",
       "      <td>1.408854</td>\n",
       "      <td>0.699587</td>\n",
       "      <td>0.731450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.295441</td>\n",
       "      <td>0.981556</td>\n",
       "      <td>0.602186</td>\n",
       "      <td>0.702342</td>\n",
       "      <td>0.345975</td>\n",
       "      <td>0.450882</td>\n",
       "      <td>0.241066</td>\n",
       "      <td>0.639303</td>\n",
       "      <td>0.551062</td>\n",
       "      <td>1.493300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>WEBHCN</td>\n",
       "      <td>10.482764</td>\n",
       "      <td>0.704114</td>\n",
       "      <td>0.162746</td>\n",
       "      <td>1.364579</td>\n",
       "      <td>1.395614</td>\n",
       "      <td>0.057514</td>\n",
       "      <td>0.404470</td>\n",
       "      <td>0.656956</td>\n",
       "      <td>0.001565</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281403</td>\n",
       "      <td>0.480669</td>\n",
       "      <td>0.594692</td>\n",
       "      <td>0.550103</td>\n",
       "      <td>1.083601</td>\n",
       "      <td>0.347547</td>\n",
       "      <td>1.216896</td>\n",
       "      <td>0.935210</td>\n",
       "      <td>0.881172</td>\n",
       "      <td>0.753875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>DJHJ91</td>\n",
       "      <td>9.797475</td>\n",
       "      <td>0.376591</td>\n",
       "      <td>0.432354</td>\n",
       "      <td>0.676720</td>\n",
       "      <td>0.362720</td>\n",
       "      <td>0.231372</td>\n",
       "      <td>0.330044</td>\n",
       "      <td>0.291869</td>\n",
       "      <td>0.104928</td>\n",
       "      <td>...</td>\n",
       "      <td>0.353732</td>\n",
       "      <td>0.450471</td>\n",
       "      <td>0.281060</td>\n",
       "      <td>0.066122</td>\n",
       "      <td>0.518536</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.379204</td>\n",
       "      <td>0.741006</td>\n",
       "      <td>0.077468</td>\n",
       "      <td>0.715211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>EQPWBM</td>\n",
       "      <td>12.286259</td>\n",
       "      <td>0.506929</td>\n",
       "      <td>0.820117</td>\n",
       "      <td>0.946161</td>\n",
       "      <td>0.663269</td>\n",
       "      <td>0.298746</td>\n",
       "      <td>0.959694</td>\n",
       "      <td>0.318049</td>\n",
       "      <td>0.527835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132259</td>\n",
       "      <td>0.744157</td>\n",
       "      <td>0.135341</td>\n",
       "      <td>0.170261</td>\n",
       "      <td>0.913241</td>\n",
       "      <td>0.262167</td>\n",
       "      <td>0.727047</td>\n",
       "      <td>0.621046</td>\n",
       "      <td>0.707463</td>\n",
       "      <td>0.361070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>P7KYZ1</td>\n",
       "      <td>5.141484</td>\n",
       "      <td>0.886230</td>\n",
       "      <td>0.319364</td>\n",
       "      <td>1.510074</td>\n",
       "      <td>1.263400</td>\n",
       "      <td>1.317618</td>\n",
       "      <td>0.061864</td>\n",
       "      <td>0.708474</td>\n",
       "      <td>1.369516</td>\n",
       "      <td>...</td>\n",
       "      <td>1.256959</td>\n",
       "      <td>0.167873</td>\n",
       "      <td>0.725021</td>\n",
       "      <td>0.912760</td>\n",
       "      <td>0.830284</td>\n",
       "      <td>1.433530</td>\n",
       "      <td>0.540819</td>\n",
       "      <td>0.381172</td>\n",
       "      <td>0.406415</td>\n",
       "      <td>0.921165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>AC7ZYL</td>\n",
       "      <td>13.293920</td>\n",
       "      <td>0.254956</td>\n",
       "      <td>0.141365</td>\n",
       "      <td>0.130070</td>\n",
       "      <td>0.216224</td>\n",
       "      <td>0.278517</td>\n",
       "      <td>0.283183</td>\n",
       "      <td>0.448955</td>\n",
       "      <td>0.402341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.191874</td>\n",
       "      <td>0.263671</td>\n",
       "      <td>0.071854</td>\n",
       "      <td>0.151124</td>\n",
       "      <td>0.206752</td>\n",
       "      <td>0.209326</td>\n",
       "      <td>0.216583</td>\n",
       "      <td>0.137522</td>\n",
       "      <td>0.264549</td>\n",
       "      <td>0.479289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>95L1AC</td>\n",
       "      <td>5.378494</td>\n",
       "      <td>0.195293</td>\n",
       "      <td>0.019657</td>\n",
       "      <td>0.370891</td>\n",
       "      <td>0.112659</td>\n",
       "      <td>0.274848</td>\n",
       "      <td>0.283908</td>\n",
       "      <td>0.300448</td>\n",
       "      <td>0.116993</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076422</td>\n",
       "      <td>0.362755</td>\n",
       "      <td>0.333100</td>\n",
       "      <td>0.079073</td>\n",
       "      <td>0.350534</td>\n",
       "      <td>0.337276</td>\n",
       "      <td>0.253120</td>\n",
       "      <td>0.016351</td>\n",
       "      <td>0.082728</td>\n",
       "      <td>0.306737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>6QE582</td>\n",
       "      <td>10.716595</td>\n",
       "      <td>0.562960</td>\n",
       "      <td>0.640266</td>\n",
       "      <td>0.776780</td>\n",
       "      <td>0.436932</td>\n",
       "      <td>0.338760</td>\n",
       "      <td>1.052205</td>\n",
       "      <td>0.986267</td>\n",
       "      <td>0.730391</td>\n",
       "      <td>...</td>\n",
       "      <td>0.704890</td>\n",
       "      <td>0.843917</td>\n",
       "      <td>0.538636</td>\n",
       "      <td>1.120183</td>\n",
       "      <td>0.986773</td>\n",
       "      <td>0.090736</td>\n",
       "      <td>0.899660</td>\n",
       "      <td>0.803504</td>\n",
       "      <td>0.923845</td>\n",
       "      <td>0.373581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>XXJP96</td>\n",
       "      <td>14.250688</td>\n",
       "      <td>0.224091</td>\n",
       "      <td>0.314234</td>\n",
       "      <td>0.046382</td>\n",
       "      <td>0.437938</td>\n",
       "      <td>0.112040</td>\n",
       "      <td>0.326019</td>\n",
       "      <td>0.319575</td>\n",
       "      <td>0.210572</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049193</td>\n",
       "      <td>0.026168</td>\n",
       "      <td>0.362053</td>\n",
       "      <td>0.127866</td>\n",
       "      <td>0.026534</td>\n",
       "      <td>0.391301</td>\n",
       "      <td>0.143576</td>\n",
       "      <td>0.204423</td>\n",
       "      <td>0.004012</td>\n",
       "      <td>0.380469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>ASSZA6</td>\n",
       "      <td>14.137815</td>\n",
       "      <td>0.902749</td>\n",
       "      <td>0.529186</td>\n",
       "      <td>0.813819</td>\n",
       "      <td>0.362784</td>\n",
       "      <td>0.095125</td>\n",
       "      <td>1.394454</td>\n",
       "      <td>1.542297</td>\n",
       "      <td>1.317934</td>\n",
       "      <td>...</td>\n",
       "      <td>1.384859</td>\n",
       "      <td>0.097049</td>\n",
       "      <td>0.896387</td>\n",
       "      <td>0.486499</td>\n",
       "      <td>0.244545</td>\n",
       "      <td>1.706895</td>\n",
       "      <td>0.344381</td>\n",
       "      <td>0.051908</td>\n",
       "      <td>0.560783</td>\n",
       "      <td>0.396384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>IJJ431</td>\n",
       "      <td>13.562687</td>\n",
       "      <td>0.985460</td>\n",
       "      <td>0.104717</td>\n",
       "      <td>0.263189</td>\n",
       "      <td>0.208364</td>\n",
       "      <td>1.169786</td>\n",
       "      <td>0.840559</td>\n",
       "      <td>1.348527</td>\n",
       "      <td>0.270030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.804306</td>\n",
       "      <td>0.395378</td>\n",
       "      <td>0.987533</td>\n",
       "      <td>1.839560</td>\n",
       "      <td>1.844161</td>\n",
       "      <td>0.791976</td>\n",
       "      <td>0.422089</td>\n",
       "      <td>0.867217</td>\n",
       "      <td>0.958783</td>\n",
       "      <td>0.229165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>63KEOW</td>\n",
       "      <td>5.626036</td>\n",
       "      <td>0.555805</td>\n",
       "      <td>0.428540</td>\n",
       "      <td>0.179723</td>\n",
       "      <td>0.224245</td>\n",
       "      <td>0.562064</td>\n",
       "      <td>1.003192</td>\n",
       "      <td>0.060349</td>\n",
       "      <td>0.140401</td>\n",
       "      <td>...</td>\n",
       "      <td>0.404829</td>\n",
       "      <td>0.898888</td>\n",
       "      <td>1.077172</td>\n",
       "      <td>0.029542</td>\n",
       "      <td>1.079818</td>\n",
       "      <td>0.304518</td>\n",
       "      <td>0.264018</td>\n",
       "      <td>0.120262</td>\n",
       "      <td>0.215621</td>\n",
       "      <td>0.570419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>EEE3WD</td>\n",
       "      <td>6.211855</td>\n",
       "      <td>0.426437</td>\n",
       "      <td>0.777724</td>\n",
       "      <td>0.255030</td>\n",
       "      <td>0.289108</td>\n",
       "      <td>0.379749</td>\n",
       "      <td>0.715273</td>\n",
       "      <td>0.052510</td>\n",
       "      <td>0.157569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.536629</td>\n",
       "      <td>0.755810</td>\n",
       "      <td>0.399345</td>\n",
       "      <td>0.537098</td>\n",
       "      <td>0.142119</td>\n",
       "      <td>0.175561</td>\n",
       "      <td>0.081939</td>\n",
       "      <td>0.519030</td>\n",
       "      <td>0.346027</td>\n",
       "      <td>0.510665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>MDYP8I</td>\n",
       "      <td>11.268894</td>\n",
       "      <td>0.984166</td>\n",
       "      <td>1.828136</td>\n",
       "      <td>1.728606</td>\n",
       "      <td>0.615685</td>\n",
       "      <td>0.133625</td>\n",
       "      <td>0.945077</td>\n",
       "      <td>0.223256</td>\n",
       "      <td>0.269456</td>\n",
       "      <td>...</td>\n",
       "      <td>0.814951</td>\n",
       "      <td>1.202369</td>\n",
       "      <td>1.638420</td>\n",
       "      <td>0.002111</td>\n",
       "      <td>1.128161</td>\n",
       "      <td>0.362846</td>\n",
       "      <td>0.029657</td>\n",
       "      <td>1.424104</td>\n",
       "      <td>0.095465</td>\n",
       "      <td>0.075454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>L0V81R</td>\n",
       "      <td>6.984658</td>\n",
       "      <td>0.263393</td>\n",
       "      <td>0.121136</td>\n",
       "      <td>0.509900</td>\n",
       "      <td>0.306134</td>\n",
       "      <td>0.510916</td>\n",
       "      <td>0.030370</td>\n",
       "      <td>0.104736</td>\n",
       "      <td>0.090282</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261962</td>\n",
       "      <td>0.061518</td>\n",
       "      <td>0.280512</td>\n",
       "      <td>0.493842</td>\n",
       "      <td>0.391207</td>\n",
       "      <td>0.434243</td>\n",
       "      <td>0.082277</td>\n",
       "      <td>0.093930</td>\n",
       "      <td>0.150280</td>\n",
       "      <td>0.454532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>ZOOTU6</td>\n",
       "      <td>13.328127</td>\n",
       "      <td>0.195890</td>\n",
       "      <td>0.183532</td>\n",
       "      <td>0.344667</td>\n",
       "      <td>0.017581</td>\n",
       "      <td>0.160755</td>\n",
       "      <td>0.314039</td>\n",
       "      <td>0.228983</td>\n",
       "      <td>0.022894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023469</td>\n",
       "      <td>0.233258</td>\n",
       "      <td>0.218390</td>\n",
       "      <td>0.279994</td>\n",
       "      <td>0.357664</td>\n",
       "      <td>0.096633</td>\n",
       "      <td>0.345367</td>\n",
       "      <td>0.312192</td>\n",
       "      <td>0.230762</td>\n",
       "      <td>0.023653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>5FO7G6</td>\n",
       "      <td>5.486292</td>\n",
       "      <td>0.826637</td>\n",
       "      <td>0.037274</td>\n",
       "      <td>1.439207</td>\n",
       "      <td>0.414294</td>\n",
       "      <td>1.295476</td>\n",
       "      <td>0.218793</td>\n",
       "      <td>0.827223</td>\n",
       "      <td>0.909116</td>\n",
       "      <td>...</td>\n",
       "      <td>1.561602</td>\n",
       "      <td>1.437679</td>\n",
       "      <td>1.357605</td>\n",
       "      <td>0.579848</td>\n",
       "      <td>0.569340</td>\n",
       "      <td>0.410159</td>\n",
       "      <td>0.653823</td>\n",
       "      <td>0.511962</td>\n",
       "      <td>0.288414</td>\n",
       "      <td>1.160512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>OWK9G7</td>\n",
       "      <td>8.898882</td>\n",
       "      <td>0.560038</td>\n",
       "      <td>0.734230</td>\n",
       "      <td>0.687237</td>\n",
       "      <td>0.103983</td>\n",
       "      <td>0.647509</td>\n",
       "      <td>1.013819</td>\n",
       "      <td>0.321764</td>\n",
       "      <td>0.726623</td>\n",
       "      <td>...</td>\n",
       "      <td>1.110149</td>\n",
       "      <td>0.347619</td>\n",
       "      <td>0.442561</td>\n",
       "      <td>1.020106</td>\n",
       "      <td>0.741502</td>\n",
       "      <td>1.012153</td>\n",
       "      <td>0.937808</td>\n",
       "      <td>0.264223</td>\n",
       "      <td>0.938752</td>\n",
       "      <td>0.238811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>U4V1VA</td>\n",
       "      <td>7.964131</td>\n",
       "      <td>0.676094</td>\n",
       "      <td>1.040505</td>\n",
       "      <td>1.267686</td>\n",
       "      <td>0.697192</td>\n",
       "      <td>0.376162</td>\n",
       "      <td>0.504609</td>\n",
       "      <td>0.613667</td>\n",
       "      <td>1.326797</td>\n",
       "      <td>...</td>\n",
       "      <td>0.600008</td>\n",
       "      <td>0.472623</td>\n",
       "      <td>0.656480</td>\n",
       "      <td>0.040984</td>\n",
       "      <td>1.141824</td>\n",
       "      <td>1.037308</td>\n",
       "      <td>1.234962</td>\n",
       "      <td>1.151604</td>\n",
       "      <td>1.309231</td>\n",
       "      <td>1.109319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>CDBSAB</td>\n",
       "      <td>11.355543</td>\n",
       "      <td>0.991058</td>\n",
       "      <td>0.367649</td>\n",
       "      <td>1.271099</td>\n",
       "      <td>0.173593</td>\n",
       "      <td>1.029194</td>\n",
       "      <td>0.146541</td>\n",
       "      <td>0.772380</td>\n",
       "      <td>0.632923</td>\n",
       "      <td>...</td>\n",
       "      <td>1.941812</td>\n",
       "      <td>0.639871</td>\n",
       "      <td>1.897366</td>\n",
       "      <td>1.345298</td>\n",
       "      <td>0.239541</td>\n",
       "      <td>1.518205</td>\n",
       "      <td>1.284733</td>\n",
       "      <td>0.842141</td>\n",
       "      <td>1.663537</td>\n",
       "      <td>1.688793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>QYAM86</td>\n",
       "      <td>7.980847</td>\n",
       "      <td>0.924642</td>\n",
       "      <td>1.291806</td>\n",
       "      <td>0.081045</td>\n",
       "      <td>1.067710</td>\n",
       "      <td>0.629870</td>\n",
       "      <td>1.181581</td>\n",
       "      <td>0.998004</td>\n",
       "      <td>0.978992</td>\n",
       "      <td>...</td>\n",
       "      <td>0.564889</td>\n",
       "      <td>1.440811</td>\n",
       "      <td>0.011676</td>\n",
       "      <td>0.179371</td>\n",
       "      <td>1.522616</td>\n",
       "      <td>0.992215</td>\n",
       "      <td>0.856132</td>\n",
       "      <td>0.944114</td>\n",
       "      <td>1.332835</td>\n",
       "      <td>1.357395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>SNP75P</td>\n",
       "      <td>14.833176</td>\n",
       "      <td>0.952576</td>\n",
       "      <td>1.278823</td>\n",
       "      <td>0.130321</td>\n",
       "      <td>0.727569</td>\n",
       "      <td>0.096593</td>\n",
       "      <td>1.617418</td>\n",
       "      <td>1.791662</td>\n",
       "      <td>0.565633</td>\n",
       "      <td>...</td>\n",
       "      <td>1.706236</td>\n",
       "      <td>1.067772</td>\n",
       "      <td>0.253282</td>\n",
       "      <td>0.640113</td>\n",
       "      <td>0.607619</td>\n",
       "      <td>1.765874</td>\n",
       "      <td>0.576517</td>\n",
       "      <td>0.983310</td>\n",
       "      <td>1.896010</td>\n",
       "      <td>1.003607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>39FKDZ</td>\n",
       "      <td>13.360809</td>\n",
       "      <td>0.884258</td>\n",
       "      <td>1.431823</td>\n",
       "      <td>0.671757</td>\n",
       "      <td>1.654451</td>\n",
       "      <td>0.648340</td>\n",
       "      <td>0.609271</td>\n",
       "      <td>0.102701</td>\n",
       "      <td>0.179480</td>\n",
       "      <td>...</td>\n",
       "      <td>0.664151</td>\n",
       "      <td>0.356764</td>\n",
       "      <td>1.157578</td>\n",
       "      <td>1.757799</td>\n",
       "      <td>0.957215</td>\n",
       "      <td>1.017477</td>\n",
       "      <td>0.872342</td>\n",
       "      <td>0.316603</td>\n",
       "      <td>0.019310</td>\n",
       "      <td>0.414654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>QH35AP</td>\n",
       "      <td>10.882680</td>\n",
       "      <td>0.690480</td>\n",
       "      <td>0.816408</td>\n",
       "      <td>0.690840</td>\n",
       "      <td>0.739302</td>\n",
       "      <td>1.312831</td>\n",
       "      <td>0.862413</td>\n",
       "      <td>0.143895</td>\n",
       "      <td>0.248601</td>\n",
       "      <td>...</td>\n",
       "      <td>0.688565</td>\n",
       "      <td>1.352199</td>\n",
       "      <td>0.750033</td>\n",
       "      <td>1.041718</td>\n",
       "      <td>0.796125</td>\n",
       "      <td>1.235440</td>\n",
       "      <td>1.050098</td>\n",
       "      <td>1.244483</td>\n",
       "      <td>1.170469</td>\n",
       "      <td>0.140048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>LFR5LZ</td>\n",
       "      <td>12.208726</td>\n",
       "      <td>0.375745</td>\n",
       "      <td>0.241236</td>\n",
       "      <td>0.656379</td>\n",
       "      <td>0.502169</td>\n",
       "      <td>0.497379</td>\n",
       "      <td>0.443420</td>\n",
       "      <td>0.065428</td>\n",
       "      <td>0.435240</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280116</td>\n",
       "      <td>0.324947</td>\n",
       "      <td>0.083793</td>\n",
       "      <td>0.098300</td>\n",
       "      <td>0.156137</td>\n",
       "      <td>0.533355</td>\n",
       "      <td>0.054632</td>\n",
       "      <td>0.512112</td>\n",
       "      <td>0.224353</td>\n",
       "      <td>0.403497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>58DWAM</td>\n",
       "      <td>7.206045</td>\n",
       "      <td>0.330324</td>\n",
       "      <td>0.410249</td>\n",
       "      <td>0.478593</td>\n",
       "      <td>0.135657</td>\n",
       "      <td>0.635068</td>\n",
       "      <td>0.353291</td>\n",
       "      <td>0.101221</td>\n",
       "      <td>0.563205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285153</td>\n",
       "      <td>0.158582</td>\n",
       "      <td>0.589563</td>\n",
       "      <td>0.553641</td>\n",
       "      <td>0.596132</td>\n",
       "      <td>0.242315</td>\n",
       "      <td>0.399763</td>\n",
       "      <td>0.414370</td>\n",
       "      <td>0.322929</td>\n",
       "      <td>0.492410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>FC8THJ</td>\n",
       "      <td>12.008205</td>\n",
       "      <td>0.953277</td>\n",
       "      <td>1.334590</td>\n",
       "      <td>1.261945</td>\n",
       "      <td>0.321470</td>\n",
       "      <td>0.998186</td>\n",
       "      <td>0.980039</td>\n",
       "      <td>1.351509</td>\n",
       "      <td>0.731511</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015137</td>\n",
       "      <td>1.325034</td>\n",
       "      <td>0.903675</td>\n",
       "      <td>1.741109</td>\n",
       "      <td>1.650749</td>\n",
       "      <td>1.483928</td>\n",
       "      <td>1.803692</td>\n",
       "      <td>1.266845</td>\n",
       "      <td>0.076171</td>\n",
       "      <td>0.087796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47 rows × 2022 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Name     Cost_1        H1        H2        H3        H4        H5  \\\n",
       "2   PV2IPX   5.038117  0.835735  1.589758  1.487095  0.983900  0.248905   \n",
       "9   T7DTIG   6.170093  0.861015  1.491885  0.388704  0.303404  0.291861   \n",
       "11  HTEDMB   5.337605  0.601614  0.717822  1.158810  0.699972  0.406064   \n",
       "12  CEPLFO  10.284382  0.671644  1.084973  1.168672  0.724060  0.310944   \n",
       "13  CNBYS3   9.356291  0.937390  0.300731  1.632274  0.908328  1.715092   \n",
       "14  2XZT21  13.258820  0.326078  0.164465  0.513230  0.422354  0.345787   \n",
       "15  DNN3DP  10.093455  0.876985  0.088443  0.383761  0.305644  0.355502   \n",
       "20  0V3A2X  14.814455  0.610612  0.191412  0.010825  0.089612  0.223186   \n",
       "21  1NIXN8  13.551261  0.968877  0.216392  0.504142  0.289065  0.575558   \n",
       "22  QW3USH  11.800536  0.831968  1.287700  0.728342  0.335391  1.435803   \n",
       "26  4KSVFB   8.973407  0.652648  0.967786  0.749000  0.785042  0.011473   \n",
       "35  USIV7B  10.284000  0.193028  0.131014  0.141899  0.144848  0.318133   \n",
       "37  2NOPEY  12.706412  0.807162  1.156811  1.404877  0.249776  0.772932   \n",
       "38  EY2ZKX   5.970391  0.845507  0.299383  0.188357  0.802668  0.446692   \n",
       "40  60EDZR   8.214620  0.869059  0.333606  0.292312  1.052218  0.560927   \n",
       "41  SPI66W   6.972012  0.737508  1.344694  1.361780  0.134995  0.585304   \n",
       "42  IO96OP  12.323025  0.786681  0.711246  1.509508  1.320507  1.243221   \n",
       "43  EBWBVY  12.572702  0.114672  0.123311  0.152474  0.138077  0.029465   \n",
       "46  0XR98F   6.586332  0.919946  1.778058  0.390447  0.278292  1.342161   \n",
       "47  3OW6RA  10.136208  0.806013  0.403378  0.138196  1.377880  0.709778   \n",
       "53  XFWIU4  10.701527  0.816055  0.382129  0.353656  0.250442  0.390416   \n",
       "55  WEBHCN  10.482764  0.704114  0.162746  1.364579  1.395614  0.057514   \n",
       "56  DJHJ91   9.797475  0.376591  0.432354  0.676720  0.362720  0.231372   \n",
       "58  EQPWBM  12.286259  0.506929  0.820117  0.946161  0.663269  0.298746   \n",
       "59  P7KYZ1   5.141484  0.886230  0.319364  1.510074  1.263400  1.317618   \n",
       "60  AC7ZYL  13.293920  0.254956  0.141365  0.130070  0.216224  0.278517   \n",
       "62  95L1AC   5.378494  0.195293  0.019657  0.370891  0.112659  0.274848   \n",
       "63  6QE582  10.716595  0.562960  0.640266  0.776780  0.436932  0.338760   \n",
       "64  XXJP96  14.250688  0.224091  0.314234  0.046382  0.437938  0.112040   \n",
       "65  ASSZA6  14.137815  0.902749  0.529186  0.813819  0.362784  0.095125   \n",
       "67  IJJ431  13.562687  0.985460  0.104717  0.263189  0.208364  1.169786   \n",
       "68  63KEOW   5.626036  0.555805  0.428540  0.179723  0.224245  0.562064   \n",
       "69  EEE3WD   6.211855  0.426437  0.777724  0.255030  0.289108  0.379749   \n",
       "73  MDYP8I  11.268894  0.984166  1.828136  1.728606  0.615685  0.133625   \n",
       "74  L0V81R   6.984658  0.263393  0.121136  0.509900  0.306134  0.510916   \n",
       "75  ZOOTU6  13.328127  0.195890  0.183532  0.344667  0.017581  0.160755   \n",
       "78  5FO7G6   5.486292  0.826637  0.037274  1.439207  0.414294  1.295476   \n",
       "79  OWK9G7   8.898882  0.560038  0.734230  0.687237  0.103983  0.647509   \n",
       "80  U4V1VA   7.964131  0.676094  1.040505  1.267686  0.697192  0.376162   \n",
       "81  CDBSAB  11.355543  0.991058  0.367649  1.271099  0.173593  1.029194   \n",
       "82  QYAM86   7.980847  0.924642  1.291806  0.081045  1.067710  0.629870   \n",
       "85  SNP75P  14.833176  0.952576  1.278823  0.130321  0.727569  0.096593   \n",
       "88  39FKDZ  13.360809  0.884258  1.431823  0.671757  1.654451  0.648340   \n",
       "93  QH35AP  10.882680  0.690480  0.816408  0.690840  0.739302  1.312831   \n",
       "95  LFR5LZ  12.208726  0.375745  0.241236  0.656379  0.502169  0.497379   \n",
       "97  58DWAM   7.206045  0.330324  0.410249  0.478593  0.135657  0.635068   \n",
       "98  FC8THJ  12.008205  0.953277  1.334590  1.261945  0.321470  0.998186   \n",
       "\n",
       "          H6        H7        H8    ...        R1991     R1992     R1993  \\\n",
       "2   0.038034  1.520773  0.565814    ...     0.374233  0.079713  0.331840   \n",
       "9   1.185245  1.110719  0.401375    ...     1.363263  0.005645  0.707717   \n",
       "11  0.765946  0.846085  0.735594    ...     0.466795  0.266191  0.717479   \n",
       "12  0.257220  1.009307  1.291811    ...     0.542760  1.312476  0.264253   \n",
       "13  1.622313  1.394172  1.437609    ...     1.632324  0.159648  0.915547   \n",
       "14  0.530717  0.006348  0.035507    ...     0.496231  0.090437  0.623189   \n",
       "15  1.045151  0.413347  0.469768    ...     0.371582  0.134904  0.560739   \n",
       "20  0.046246  0.350807  0.338857    ...     0.427678  0.458192  0.109410   \n",
       "21  1.225706  1.832520  0.696728    ...     0.210890  0.744018  0.974609   \n",
       "22  0.851525  0.201946  0.342744    ...     1.148539  1.456334  0.464729   \n",
       "26  1.186318  0.290621  1.133813    ...     1.256736  1.146073  0.990398   \n",
       "35  0.256971  0.277086  0.029861    ...     0.142844  0.050576  0.221908   \n",
       "37  0.996842  0.714230  1.231330    ...     1.120322  0.248247  0.130342   \n",
       "38  0.162821  0.471423  0.701029    ...     1.250464  1.016952  1.493628   \n",
       "40  0.832845  1.423188  0.374126    ...     1.605534  1.226621  1.662906   \n",
       "41  0.675788  1.279265  0.945051    ...     0.477370  0.765020  0.838403   \n",
       "42  1.184888  1.404887  1.151514    ...     0.541593  0.442798  1.532835   \n",
       "43  0.020562  0.206288  0.144936    ...     0.099471  0.018669  0.066881   \n",
       "46  0.923665  0.771908  0.795025    ...     0.860271  0.216450  0.436777   \n",
       "47  1.301230  0.925927  0.374428    ...     0.051531  1.023437  1.022839   \n",
       "53  1.408854  0.699587  0.731450    ...     0.295441  0.981556  0.602186   \n",
       "55  0.404470  0.656956  0.001565    ...     0.281403  0.480669  0.594692   \n",
       "56  0.330044  0.291869  0.104928    ...     0.353732  0.450471  0.281060   \n",
       "58  0.959694  0.318049  0.527835    ...     0.132259  0.744157  0.135341   \n",
       "59  0.061864  0.708474  1.369516    ...     1.256959  0.167873  0.725021   \n",
       "60  0.283183  0.448955  0.402341    ...     0.191874  0.263671  0.071854   \n",
       "62  0.283908  0.300448  0.116993    ...     0.076422  0.362755  0.333100   \n",
       "63  1.052205  0.986267  0.730391    ...     0.704890  0.843917  0.538636   \n",
       "64  0.326019  0.319575  0.210572    ...     0.049193  0.026168  0.362053   \n",
       "65  1.394454  1.542297  1.317934    ...     1.384859  0.097049  0.896387   \n",
       "67  0.840559  1.348527  0.270030    ...     0.804306  0.395378  0.987533   \n",
       "68  1.003192  0.060349  0.140401    ...     0.404829  0.898888  1.077172   \n",
       "69  0.715273  0.052510  0.157569    ...     0.536629  0.755810  0.399345   \n",
       "73  0.945077  0.223256  0.269456    ...     0.814951  1.202369  1.638420   \n",
       "74  0.030370  0.104736  0.090282    ...     0.261962  0.061518  0.280512   \n",
       "75  0.314039  0.228983  0.022894    ...     0.023469  0.233258  0.218390   \n",
       "78  0.218793  0.827223  0.909116    ...     1.561602  1.437679  1.357605   \n",
       "79  1.013819  0.321764  0.726623    ...     1.110149  0.347619  0.442561   \n",
       "80  0.504609  0.613667  1.326797    ...     0.600008  0.472623  0.656480   \n",
       "81  0.146541  0.772380  0.632923    ...     1.941812  0.639871  1.897366   \n",
       "82  1.181581  0.998004  0.978992    ...     0.564889  1.440811  0.011676   \n",
       "85  1.617418  1.791662  0.565633    ...     1.706236  1.067772  0.253282   \n",
       "88  0.609271  0.102701  0.179480    ...     0.664151  0.356764  1.157578   \n",
       "93  0.862413  0.143895  0.248601    ...     0.688565  1.352199  0.750033   \n",
       "95  0.443420  0.065428  0.435240    ...     0.280116  0.324947  0.083793   \n",
       "97  0.353291  0.101221  0.563205    ...     0.285153  0.158582  0.589563   \n",
       "98  0.980039  1.351509  0.731511    ...     0.015137  1.325034  0.903675   \n",
       "\n",
       "       R1994     R1995     R1996     R1997     R1998     R1999     R2000  \n",
       "2   0.445684  1.014169  0.446604  1.307291  0.501331  1.395232  0.567960  \n",
       "9   1.441140  0.230907  1.123617  0.363932  1.323259  1.092587  1.146443  \n",
       "11  0.150243  1.041188  0.882118  1.036286  0.145335  0.402870  0.185906  \n",
       "12  1.048437  1.063177  0.150586  0.940793  0.710916  0.823709  0.440530  \n",
       "13  0.612437  1.412655  1.162172  0.812511  1.351329  0.532182  0.756806  \n",
       "14  0.599199  0.246096  0.592078  0.512876  0.089486  0.282901  0.463123  \n",
       "15  1.476387  0.303390  0.390836  0.348461  1.670531  0.517499  1.505587  \n",
       "20  0.658182  0.767887  0.256473  1.043085  0.068918  0.523834  0.979901  \n",
       "21  0.960943  1.780807  1.288110  0.717676  1.378187  1.637874  0.619832  \n",
       "22  1.087713  0.007711  0.364899  0.705076  1.505881  0.625855  0.510318  \n",
       "26  0.204778  1.025945  1.137440  0.898531  0.080849  1.101648  1.163442  \n",
       "35  0.258092  0.163441  0.264920  0.072136  0.373083  0.185653  0.061511  \n",
       "37  0.385066  0.242253  0.868283  0.003301  0.060758  1.122132  0.462768  \n",
       "38  1.644986  0.348333  1.295400  0.243640  1.393367  0.365204  1.356741  \n",
       "40  0.665913  0.968328  1.036657  0.248921  1.478777  0.994675  1.269654  \n",
       "41  0.225098  1.049114  0.107519  1.225292  0.114809  0.740481  0.351209  \n",
       "42  1.118471  0.230931  1.558875  1.123411  1.525556  1.443788  1.440434  \n",
       "43  0.181942  0.197919  0.000990  0.021245  0.062013  0.198201  0.002924  \n",
       "46  1.804138  1.169294  1.742070  1.100735  0.986341  0.623001  0.052091  \n",
       "47  0.001232  1.518495  1.193740  1.402546  1.096816  0.203422  0.364915  \n",
       "53  0.702342  0.345975  0.450882  0.241066  0.639303  0.551062  1.493300  \n",
       "55  0.550103  1.083601  0.347547  1.216896  0.935210  0.881172  0.753875  \n",
       "56  0.066122  0.518536  0.685714  0.379204  0.741006  0.077468  0.715211  \n",
       "58  0.170261  0.913241  0.262167  0.727047  0.621046  0.707463  0.361070  \n",
       "59  0.912760  0.830284  1.433530  0.540819  0.381172  0.406415  0.921165  \n",
       "60  0.151124  0.206752  0.209326  0.216583  0.137522  0.264549  0.479289  \n",
       "62  0.079073  0.350534  0.337276  0.253120  0.016351  0.082728  0.306737  \n",
       "63  1.120183  0.986773  0.090736  0.899660  0.803504  0.923845  0.373581  \n",
       "64  0.127866  0.026534  0.391301  0.143576  0.204423  0.004012  0.380469  \n",
       "65  0.486499  0.244545  1.706895  0.344381  0.051908  0.560783  0.396384  \n",
       "67  1.839560  1.844161  0.791976  0.422089  0.867217  0.958783  0.229165  \n",
       "68  0.029542  1.079818  0.304518  0.264018  0.120262  0.215621  0.570419  \n",
       "69  0.537098  0.142119  0.175561  0.081939  0.519030  0.346027  0.510665  \n",
       "73  0.002111  1.128161  0.362846  0.029657  1.424104  0.095465  0.075454  \n",
       "74  0.493842  0.391207  0.434243  0.082277  0.093930  0.150280  0.454532  \n",
       "75  0.279994  0.357664  0.096633  0.345367  0.312192  0.230762  0.023653  \n",
       "78  0.579848  0.569340  0.410159  0.653823  0.511962  0.288414  1.160512  \n",
       "79  1.020106  0.741502  1.012153  0.937808  0.264223  0.938752  0.238811  \n",
       "80  0.040984  1.141824  1.037308  1.234962  1.151604  1.309231  1.109319  \n",
       "81  1.345298  0.239541  1.518205  1.284733  0.842141  1.663537  1.688793  \n",
       "82  0.179371  1.522616  0.992215  0.856132  0.944114  1.332835  1.357395  \n",
       "85  0.640113  0.607619  1.765874  0.576517  0.983310  1.896010  1.003607  \n",
       "88  1.757799  0.957215  1.017477  0.872342  0.316603  0.019310  0.414654  \n",
       "93  1.041718  0.796125  1.235440  1.050098  1.244483  1.170469  0.140048  \n",
       "95  0.098300  0.156137  0.533355  0.054632  0.512112  0.224353  0.403497  \n",
       "97  0.553641  0.596132  0.242315  0.399763  0.414370  0.322929  0.492410  \n",
       "98  1.741109  1.650749  1.483928  1.803692  1.266845  0.076171  0.087796  \n",
       "\n",
       "[47 rows x 2022 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=dataset.loc[k,:]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reward of trial t will be revealed at trial t+1\n",
    "#generate reward\n",
    "historical_reward=[]\n",
    "present_reward=[]\n",
    "\n",
    "for t in range(1,21):\n",
    "   # historical_reward.append([])\n",
    "    #m=[]\n",
    "    #for i in range(0,len(k)):\n",
    "    historical_reward.append((4/3)*(1+((1/(np.pi))*(np.arctan(np.log(df.loc[:,'H'+str(t)])))))**2-(4/3))\n",
    "for t in range(1,no_of_trials+1):\n",
    "    present_reward.append((4/3)*(1+((1/(np.pi))*(np.arctan(np.log(df.loc[:,'R'+str(t)])))))**2-(4/3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#historical_reward[1]\n",
    "#print(k)\n",
    "#present_reward\n",
    "#historical_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>H1</th>\n",
       "      <th>H2</th>\n",
       "      <th>H3</th>\n",
       "      <th>H4</th>\n",
       "      <th>H5</th>\n",
       "      <th>H6</th>\n",
       "      <th>H7</th>\n",
       "      <th>H8</th>\n",
       "      <th>H9</th>\n",
       "      <th>H10</th>\n",
       "      <th>...</th>\n",
       "      <th>R1991</th>\n",
       "      <th>R1992</th>\n",
       "      <th>R1993</th>\n",
       "      <th>R1994</th>\n",
       "      <th>R1995</th>\n",
       "      <th>R1996</th>\n",
       "      <th>R1997</th>\n",
       "      <th>R1998</th>\n",
       "      <th>R1999</th>\n",
       "      <th>R2000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.146454</td>\n",
       "      <td>0.393924</td>\n",
       "      <td>0.339937</td>\n",
       "      <td>-0.013740</td>\n",
       "      <td>-0.682913</td>\n",
       "      <td>-0.862113</td>\n",
       "      <td>0.358242</td>\n",
       "      <td>-0.403218</td>\n",
       "      <td>0.343756</td>\n",
       "      <td>0.032275</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.577826</td>\n",
       "      <td>-0.821057</td>\n",
       "      <td>-0.614194</td>\n",
       "      <td>-0.514526</td>\n",
       "      <td>0.011969</td>\n",
       "      <td>-0.513696</td>\n",
       "      <td>0.231489</td>\n",
       "      <td>-0.463622</td>\n",
       "      <td>0.286866</td>\n",
       "      <td>-0.401187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.123105</td>\n",
       "      <td>0.342577</td>\n",
       "      <td>-0.565207</td>\n",
       "      <td>-0.638097</td>\n",
       "      <td>-0.647693</td>\n",
       "      <td>0.146720</td>\n",
       "      <td>0.090287</td>\n",
       "      <td>-0.554072</td>\n",
       "      <td>-0.629386</td>\n",
       "      <td>0.103788</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267269</td>\n",
       "      <td>-0.914097</td>\n",
       "      <td>-0.267564</td>\n",
       "      <td>0.313976</td>\n",
       "      <td>-0.697473</td>\n",
       "      <td>0.100308</td>\n",
       "      <td>-0.586745</td>\n",
       "      <td>0.241890</td>\n",
       "      <td>0.076020</td>\n",
       "      <td>0.117782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.369206</td>\n",
       "      <td>-0.257890</td>\n",
       "      <td>0.127110</td>\n",
       "      <td>-0.274984</td>\n",
       "      <td>-0.549932</td>\n",
       "      <td>-0.212014</td>\n",
       "      <td>-0.136864</td>\n",
       "      <td>-0.240905</td>\n",
       "      <td>-0.226033</td>\n",
       "      <td>-0.778029</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.495367</td>\n",
       "      <td>-0.668827</td>\n",
       "      <td>-0.258218</td>\n",
       "      <td>-0.762129</td>\n",
       "      <td>0.034462</td>\n",
       "      <td>-0.103812</td>\n",
       "      <td>0.030413</td>\n",
       "      <td>-0.766085</td>\n",
       "      <td>-0.552753</td>\n",
       "      <td>-0.733558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.302154</td>\n",
       "      <td>0.069967</td>\n",
       "      <td>0.134479</td>\n",
       "      <td>-0.251923</td>\n",
       "      <td>-0.631796</td>\n",
       "      <td>-0.676151</td>\n",
       "      <td>0.007875</td>\n",
       "      <td>0.221255</td>\n",
       "      <td>-0.342874</td>\n",
       "      <td>-0.312439</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.424961</td>\n",
       "      <td>0.234883</td>\n",
       "      <td>-0.670412</td>\n",
       "      <td>0.040422</td>\n",
       "      <td>0.052441</td>\n",
       "      <td>-0.761853</td>\n",
       "      <td>-0.051239</td>\n",
       "      <td>-0.264500</td>\n",
       "      <td>-0.157644</td>\n",
       "      <td>-0.519173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.054242</td>\n",
       "      <td>-0.640325</td>\n",
       "      <td>0.414762</td>\n",
       "      <td>-0.080123</td>\n",
       "      <td>0.452996</td>\n",
       "      <td>0.409956</td>\n",
       "      <td>0.286226</td>\n",
       "      <td>0.311934</td>\n",
       "      <td>0.290435</td>\n",
       "      <td>-0.535879</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414786</td>\n",
       "      <td>-0.754571</td>\n",
       "      <td>-0.073655</td>\n",
       "      <td>-0.358876</td>\n",
       "      <td>0.297296</td>\n",
       "      <td>0.129629</td>\n",
       "      <td>-0.168108</td>\n",
       "      <td>0.259798</td>\n",
       "      <td>-0.434886</td>\n",
       "      <td>-0.220696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.619068</td>\n",
       "      <td>-0.750709</td>\n",
       "      <td>-0.452578</td>\n",
       "      <td>-0.535464</td>\n",
       "      <td>-0.602328</td>\n",
       "      <td>-0.436257</td>\n",
       "      <td>-0.912040</td>\n",
       "      <td>-0.865021</td>\n",
       "      <td>-0.825729</td>\n",
       "      <td>-0.672619</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.468339</td>\n",
       "      <td>-0.811633</td>\n",
       "      <td>-0.348598</td>\n",
       "      <td>-0.371508</td>\n",
       "      <td>-0.685192</td>\n",
       "      <td>-0.378291</td>\n",
       "      <td>-0.452907</td>\n",
       "      <td>-0.812458</td>\n",
       "      <td>-0.655101</td>\n",
       "      <td>-0.498714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.108487</td>\n",
       "      <td>-0.813365</td>\n",
       "      <td>-0.569529</td>\n",
       "      <td>-0.636228</td>\n",
       "      <td>-0.594005</td>\n",
       "      <td>0.037724</td>\n",
       "      <td>-0.543479</td>\n",
       "      <td>-0.492653</td>\n",
       "      <td>-0.171989</td>\n",
       "      <td>0.155846</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.580126</td>\n",
       "      <td>-0.774531</td>\n",
       "      <td>-0.408017</td>\n",
       "      <td>0.333990</td>\n",
       "      <td>-0.638109</td>\n",
       "      <td>-0.563338</td>\n",
       "      <td>-0.600042</td>\n",
       "      <td>0.432799</td>\n",
       "      <td>-0.448603</td>\n",
       "      <td>0.350062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.360619</td>\n",
       "      <td>-0.729156</td>\n",
       "      <td>-0.901321</td>\n",
       "      <td>-0.812348</td>\n",
       "      <td>-0.703690</td>\n",
       "      <td>-0.853144</td>\n",
       "      <td>-0.598033</td>\n",
       "      <td>-0.608235</td>\n",
       "      <td>-0.127305</td>\n",
       "      <td>-0.060557</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.530709</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>-0.795508</td>\n",
       "      <td>-0.315068</td>\n",
       "      <td>-0.210174</td>\n",
       "      <td>-0.676759</td>\n",
       "      <td>0.036025</td>\n",
       "      <td>-0.830868</td>\n",
       "      <td>-0.442693</td>\n",
       "      <td>-0.017176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.026694</td>\n",
       "      <td>-0.709150</td>\n",
       "      <td>-0.461017</td>\n",
       "      <td>-0.650008</td>\n",
       "      <td>-0.393988</td>\n",
       "      <td>0.175869</td>\n",
       "      <td>0.502333</td>\n",
       "      <td>-0.278094</td>\n",
       "      <td>0.345569</td>\n",
       "      <td>-0.441543</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.713565</td>\n",
       "      <td>-0.232869</td>\n",
       "      <td>-0.021737</td>\n",
       "      <td>-0.033586</td>\n",
       "      <td>0.481271</td>\n",
       "      <td>0.218786</td>\n",
       "      <td>-0.258029</td>\n",
       "      <td>0.276492</td>\n",
       "      <td>0.417444</td>\n",
       "      <td>-0.351808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.149954</td>\n",
       "      <td>0.218512</td>\n",
       "      <td>-0.247831</td>\n",
       "      <td>-0.611182</td>\n",
       "      <td>0.310886</td>\n",
       "      <td>-0.131841</td>\n",
       "      <td>-0.720729</td>\n",
       "      <td>-0.604925</td>\n",
       "      <td>0.121482</td>\n",
       "      <td>0.357922</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119370</td>\n",
       "      <td>0.322688</td>\n",
       "      <td>-0.497251</td>\n",
       "      <td>0.072150</td>\n",
       "      <td>-0.908411</td>\n",
       "      <td>-0.585910</td>\n",
       "      <td>-0.270094</td>\n",
       "      <td>0.350221</td>\n",
       "      <td>-0.346046</td>\n",
       "      <td>-0.455285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.320377</td>\n",
       "      <td>-0.027639</td>\n",
       "      <td>-0.228123</td>\n",
       "      <td>-0.193938</td>\n",
       "      <td>-0.899995</td>\n",
       "      <td>0.147507</td>\n",
       "      <td>-0.648720</td>\n",
       "      <td>0.108155</td>\n",
       "      <td>-0.032898</td>\n",
       "      <td>-0.833387</td>\n",
       "      <td>...</td>\n",
       "      <td>0.197516</td>\n",
       "      <td>0.117502</td>\n",
       "      <td>-0.008177</td>\n",
       "      <td>-0.718462</td>\n",
       "      <td>0.021826</td>\n",
       "      <td>0.110930</td>\n",
       "      <td>-0.088941</td>\n",
       "      <td>-0.820045</td>\n",
       "      <td>0.083175</td>\n",
       "      <td>0.130579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-0.727864</td>\n",
       "      <td>-0.777697</td>\n",
       "      <td>-0.768861</td>\n",
       "      <td>-0.766478</td>\n",
       "      <td>-0.625764</td>\n",
       "      <td>-0.676353</td>\n",
       "      <td>-0.659890</td>\n",
       "      <td>-0.871844</td>\n",
       "      <td>-0.753360</td>\n",
       "      <td>-0.860117</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.768097</td>\n",
       "      <td>-0.848651</td>\n",
       "      <td>-0.704718</td>\n",
       "      <td>-0.675440</td>\n",
       "      <td>-0.751530</td>\n",
       "      <td>-0.669866</td>\n",
       "      <td>-0.827903</td>\n",
       "      <td>-0.578824</td>\n",
       "      <td>-0.733760</td>\n",
       "      <td>-0.837847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-0.173121</td>\n",
       "      <td>0.125609</td>\n",
       "      <td>0.292662</td>\n",
       "      <td>-0.682206</td>\n",
       "      <td>-0.205392</td>\n",
       "      <td>-0.002684</td>\n",
       "      <td>-0.261327</td>\n",
       "      <td>0.179838</td>\n",
       "      <td>-0.556664</td>\n",
       "      <td>0.251159</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097758</td>\n",
       "      <td>-0.683447</td>\n",
       "      <td>-0.778245</td>\n",
       "      <td>-0.568389</td>\n",
       "      <td>-0.688306</td>\n",
       "      <td>-0.116439</td>\n",
       "      <td>-0.922407</td>\n",
       "      <td>-0.838570</td>\n",
       "      <td>0.099159</td>\n",
       "      <td>-0.499037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.137399</td>\n",
       "      <td>-0.641446</td>\n",
       "      <td>-0.731599</td>\n",
       "      <td>-0.177338</td>\n",
       "      <td>-0.513616</td>\n",
       "      <td>-0.752027</td>\n",
       "      <td>-0.491141</td>\n",
       "      <td>-0.273972</td>\n",
       "      <td>-0.782325</td>\n",
       "      <td>0.232018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.193190</td>\n",
       "      <td>0.014305</td>\n",
       "      <td>0.343534</td>\n",
       "      <td>0.420829</td>\n",
       "      <td>-0.600151</td>\n",
       "      <td>0.223641</td>\n",
       "      <td>-0.687182</td>\n",
       "      <td>0.285739</td>\n",
       "      <td>-0.585647</td>\n",
       "      <td>0.263197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.115728</td>\n",
       "      <td>-0.612697</td>\n",
       "      <td>-0.647319</td>\n",
       "      <td>0.043518</td>\n",
       "      <td>-0.407839</td>\n",
       "      <td>-0.149138</td>\n",
       "      <td>0.303518</td>\n",
       "      <td>-0.577919</td>\n",
       "      <td>0.369182</td>\n",
       "      <td>-0.501608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.401756</td>\n",
       "      <td>0.176516</td>\n",
       "      <td>0.429257</td>\n",
       "      <td>-0.307652</td>\n",
       "      <td>-0.027169</td>\n",
       "      <td>0.030720</td>\n",
       "      <td>-0.682900</td>\n",
       "      <td>0.335323</td>\n",
       "      <td>-0.004529</td>\n",
       "      <td>0.206348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-0.239078</td>\n",
       "      <td>0.255608</td>\n",
       "      <td>0.266345</td>\n",
       "      <td>-0.774457</td>\n",
       "      <td>-0.384734</td>\n",
       "      <td>-0.298178</td>\n",
       "      <td>0.212852</td>\n",
       "      <td>-0.047491</td>\n",
       "      <td>0.241916</td>\n",
       "      <td>0.241586</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.485698</td>\n",
       "      <td>-0.212894</td>\n",
       "      <td>-0.143978</td>\n",
       "      <td>-0.702152</td>\n",
       "      <td>0.040977</td>\n",
       "      <td>-0.797091</td>\n",
       "      <td>0.175576</td>\n",
       "      <td>-0.791013</td>\n",
       "      <td>-0.236242</td>\n",
       "      <td>-0.597688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.192391</td>\n",
       "      <td>-0.264184</td>\n",
       "      <td>0.352185</td>\n",
       "      <td>0.240108</td>\n",
       "      <td>0.188163</td>\n",
       "      <td>0.146459</td>\n",
       "      <td>0.292668</td>\n",
       "      <td>0.121619</td>\n",
       "      <td>0.322182</td>\n",
       "      <td>0.322255</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.426058</td>\n",
       "      <td>-0.517130</td>\n",
       "      <td>0.364654</td>\n",
       "      <td>0.096322</td>\n",
       "      <td>-0.697453</td>\n",
       "      <td>0.378244</td>\n",
       "      <td>0.100149</td>\n",
       "      <td>0.360793</td>\n",
       "      <td>0.315504</td>\n",
       "      <td>0.313568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.791127</td>\n",
       "      <td>-0.783998</td>\n",
       "      <td>-0.760333</td>\n",
       "      <td>-0.771955</td>\n",
       "      <td>-0.872342</td>\n",
       "      <td>-0.884482</td>\n",
       "      <td>-0.717253</td>\n",
       "      <td>-0.766407</td>\n",
       "      <td>-0.853865</td>\n",
       "      <td>-0.881081</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.803882</td>\n",
       "      <td>-0.887366</td>\n",
       "      <td>-0.832764</td>\n",
       "      <td>-0.736727</td>\n",
       "      <td>-0.723952</td>\n",
       "      <td>-0.936283</td>\n",
       "      <td>-0.883473</td>\n",
       "      <td>-0.837366</td>\n",
       "      <td>-0.723727</td>\n",
       "      <td>-0.924070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-0.069726</td>\n",
       "      <td>0.480122</td>\n",
       "      <td>-0.563680</td>\n",
       "      <td>-0.658898</td>\n",
       "      <td>0.254001</td>\n",
       "      <td>-0.066413</td>\n",
       "      <td>-0.206361</td>\n",
       "      <td>-0.184525</td>\n",
       "      <td>0.481877</td>\n",
       "      <td>-0.046445</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.123788</td>\n",
       "      <td>-0.709103</td>\n",
       "      <td>-0.522550</td>\n",
       "      <td>0.490898</td>\n",
       "      <td>0.134941</td>\n",
       "      <td>0.464815</td>\n",
       "      <td>0.082457</td>\n",
       "      <td>-0.011648</td>\n",
       "      <td>-0.348777</td>\n",
       "      <td>-0.847112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-0.174198</td>\n",
       "      <td>-0.552304</td>\n",
       "      <td>-0.771860</td>\n",
       "      <td>0.276304</td>\n",
       "      <td>-0.265590</td>\n",
       "      <td>0.227499</td>\n",
       "      <td>-0.064400</td>\n",
       "      <td>-0.577656</td>\n",
       "      <td>-0.868770</td>\n",
       "      <td>-0.474837</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.847679</td>\n",
       "      <td>0.019733</td>\n",
       "      <td>0.019234</td>\n",
       "      <td>-0.934142</td>\n",
       "      <td>0.357022</td>\n",
       "      <td>0.152928</td>\n",
       "      <td>0.291266</td>\n",
       "      <td>0.079366</td>\n",
       "      <td>-0.719548</td>\n",
       "      <td>-0.585896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>-0.164791</td>\n",
       "      <td>-0.570953</td>\n",
       "      <td>-0.595590</td>\n",
       "      <td>-0.681665</td>\n",
       "      <td>-0.563707</td>\n",
       "      <td>0.295036</td>\n",
       "      <td>-0.275353</td>\n",
       "      <td>-0.244861</td>\n",
       "      <td>-0.108978</td>\n",
       "      <td>0.350997</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.644723</td>\n",
       "      <td>-0.015753</td>\n",
       "      <td>-0.368660</td>\n",
       "      <td>-0.272713</td>\n",
       "      <td>-0.602167</td>\n",
       "      <td>-0.509827</td>\n",
       "      <td>-0.689267</td>\n",
       "      <td>-0.333169</td>\n",
       "      <td>-0.417147</td>\n",
       "      <td>0.343354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>-0.271016</td>\n",
       "      <td>-0.752087</td>\n",
       "      <td>0.268088</td>\n",
       "      <td>0.287097</td>\n",
       "      <td>-0.841718</td>\n",
       "      <td>-0.551340</td>\n",
       "      <td>-0.316245</td>\n",
       "      <td>-0.931633</td>\n",
       "      <td>-0.736283</td>\n",
       "      <td>-0.126570</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.656337</td>\n",
       "      <td>-0.482672</td>\n",
       "      <td>-0.375802</td>\n",
       "      <td>-0.418051</td>\n",
       "      <td>0.068873</td>\n",
       "      <td>-0.600823</td>\n",
       "      <td>0.169611</td>\n",
       "      <td>-0.056169</td>\n",
       "      <td>-0.104672</td>\n",
       "      <td>-0.223484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>-0.575777</td>\n",
       "      <td>-0.526521</td>\n",
       "      <td>-0.297284</td>\n",
       "      <td>-0.587791</td>\n",
       "      <td>-0.697098</td>\n",
       "      <td>-0.615715</td>\n",
       "      <td>-0.647686</td>\n",
       "      <td>-0.799267</td>\n",
       "      <td>-0.611824</td>\n",
       "      <td>-0.258940</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.595525</td>\n",
       "      <td>-0.510200</td>\n",
       "      <td>-0.656619</td>\n",
       "      <td>-0.833475</td>\n",
       "      <td>-0.447636</td>\n",
       "      <td>-0.288656</td>\n",
       "      <td>-0.573503</td>\n",
       "      <td>-0.235741</td>\n",
       "      <td>-0.823067</td>\n",
       "      <td>-0.260389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>-0.458432</td>\n",
       "      <td>-0.160996</td>\n",
       "      <td>-0.046515</td>\n",
       "      <td>-0.310189</td>\n",
       "      <td>-0.641977</td>\n",
       "      <td>-0.034673</td>\n",
       "      <td>-0.625834</td>\n",
       "      <td>-0.438954</td>\n",
       "      <td>-0.756202</td>\n",
       "      <td>-0.724667</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.776682</td>\n",
       "      <td>-0.232737</td>\n",
       "      <td>-0.774176</td>\n",
       "      <td>-0.746069</td>\n",
       "      <td>-0.075718</td>\n",
       "      <td>-0.672116</td>\n",
       "      <td>-0.249068</td>\n",
       "      <td>-0.350647</td>\n",
       "      <td>-0.267807</td>\n",
       "      <td>-0.589213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>-0.100074</td>\n",
       "      <td>-0.624728</td>\n",
       "      <td>0.352491</td>\n",
       "      <td>0.202085</td>\n",
       "      <td>0.238233</td>\n",
       "      <td>-0.837509</td>\n",
       "      <td>-0.266839</td>\n",
       "      <td>0.271150</td>\n",
       "      <td>-0.014574</td>\n",
       "      <td>0.190013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.197669</td>\n",
       "      <td>-0.747980</td>\n",
       "      <td>-0.251005</td>\n",
       "      <td>-0.076150</td>\n",
       "      <td>-0.151520</td>\n",
       "      <td>0.309565</td>\n",
       "      <td>-0.426784</td>\n",
       "      <td>-0.571788</td>\n",
       "      <td>-0.549621</td>\n",
       "      <td>-0.068640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>-0.677994</td>\n",
       "      <td>-0.769293</td>\n",
       "      <td>-0.778466</td>\n",
       "      <td>-0.709285</td>\n",
       "      <td>-0.658713</td>\n",
       "      <td>-0.654868</td>\n",
       "      <td>-0.511571</td>\n",
       "      <td>-0.553220</td>\n",
       "      <td>-0.700434</td>\n",
       "      <td>-0.804784</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.728787</td>\n",
       "      <td>-0.670887</td>\n",
       "      <td>-0.828161</td>\n",
       "      <td>-0.761420</td>\n",
       "      <td>-0.716881</td>\n",
       "      <td>-0.714818</td>\n",
       "      <td>-0.708997</td>\n",
       "      <td>-0.772406</td>\n",
       "      <td>-0.670170</td>\n",
       "      <td>-0.483938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>-0.726053</td>\n",
       "      <td>-0.885843</td>\n",
       "      <td>-0.580725</td>\n",
       "      <td>-0.792799</td>\n",
       "      <td>-0.661731</td>\n",
       "      <td>-0.654271</td>\n",
       "      <td>-0.640560</td>\n",
       "      <td>-0.789205</td>\n",
       "      <td>-0.812468</td>\n",
       "      <td>-0.671632</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.824009</td>\n",
       "      <td>-0.587760</td>\n",
       "      <td>-0.613126</td>\n",
       "      <td>-0.821629</td>\n",
       "      <td>-0.598267</td>\n",
       "      <td>-0.609580</td>\n",
       "      <td>-0.679488</td>\n",
       "      <td>-0.891103</td>\n",
       "      <td>-0.818379</td>\n",
       "      <td>-0.635315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>-0.405918</td>\n",
       "      <td>-0.332246</td>\n",
       "      <td>-0.201748</td>\n",
       "      <td>-0.522411</td>\n",
       "      <td>-0.608318</td>\n",
       "      <td>0.043507</td>\n",
       "      <td>-0.011711</td>\n",
       "      <td>-0.245873</td>\n",
       "      <td>-0.572549</td>\n",
       "      <td>-0.549952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.270272</td>\n",
       "      <td>-0.138870</td>\n",
       "      <td>-0.428834</td>\n",
       "      <td>0.097650</td>\n",
       "      <td>-0.011277</td>\n",
       "      <td>-0.811374</td>\n",
       "      <td>-0.087922</td>\n",
       "      <td>-0.176553</td>\n",
       "      <td>-0.066252</td>\n",
       "      <td>-0.578392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>-0.702962</td>\n",
       "      <td>-0.629039</td>\n",
       "      <td>-0.853000</td>\n",
       "      <td>-0.521506</td>\n",
       "      <td>-0.793314</td>\n",
       "      <td>-0.619118</td>\n",
       "      <td>-0.624551</td>\n",
       "      <td>-0.713820</td>\n",
       "      <td>-0.574140</td>\n",
       "      <td>-0.604150</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.850071</td>\n",
       "      <td>-0.876611</td>\n",
       "      <td>-0.588366</td>\n",
       "      <td>-0.780267</td>\n",
       "      <td>-0.876126</td>\n",
       "      <td>-0.562931</td>\n",
       "      <td>-0.767505</td>\n",
       "      <td>-0.718746</td>\n",
       "      <td>-0.919576</td>\n",
       "      <td>-0.572401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>-0.085139</td>\n",
       "      <td>-0.437690</td>\n",
       "      <td>-0.166883</td>\n",
       "      <td>-0.587736</td>\n",
       "      <td>-0.807592</td>\n",
       "      <td>0.286397</td>\n",
       "      <td>0.369632</td>\n",
       "      <td>0.238438</td>\n",
       "      <td>0.348872</td>\n",
       "      <td>0.050215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280573</td>\n",
       "      <td>-0.805946</td>\n",
       "      <td>-0.090876</td>\n",
       "      <td>-0.477314</td>\n",
       "      <td>-0.686449</td>\n",
       "      <td>0.449345</td>\n",
       "      <td>-0.603528</td>\n",
       "      <td>-0.847297</td>\n",
       "      <td>-0.407975</td>\n",
       "      <td>-0.558467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>-0.012403</td>\n",
       "      <td>-0.799444</td>\n",
       "      <td>-0.671281</td>\n",
       "      <td>-0.715589</td>\n",
       "      <td>0.135307</td>\n",
       "      <td>-0.141979</td>\n",
       "      <td>0.258032</td>\n",
       "      <td>-0.665683</td>\n",
       "      <td>-0.890065</td>\n",
       "      <td>-0.545057</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.175800</td>\n",
       "      <td>-0.559352</td>\n",
       "      <td>-0.010627</td>\n",
       "      <td>0.505123</td>\n",
       "      <td>0.506937</td>\n",
       "      <td>-0.187398</td>\n",
       "      <td>-0.535700</td>\n",
       "      <td>-0.117415</td>\n",
       "      <td>-0.035468</td>\n",
       "      <td>-0.698877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>-0.412675</td>\n",
       "      <td>-0.529937</td>\n",
       "      <td>-0.738500</td>\n",
       "      <td>-0.702838</td>\n",
       "      <td>-0.406765</td>\n",
       "      <td>0.002707</td>\n",
       "      <td>-0.838964</td>\n",
       "      <td>-0.770073</td>\n",
       "      <td>-0.474638</td>\n",
       "      <td>-0.821904</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.551023</td>\n",
       "      <td>-0.088619</td>\n",
       "      <td>0.063729</td>\n",
       "      <td>-0.872245</td>\n",
       "      <td>0.065849</td>\n",
       "      <td>-0.637168</td>\n",
       "      <td>-0.670604</td>\n",
       "      <td>-0.786506</td>\n",
       "      <td>-0.709769</td>\n",
       "      <td>-0.398859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>-0.531818</td>\n",
       "      <td>-0.200855</td>\n",
       "      <td>-0.677934</td>\n",
       "      <td>-0.649973</td>\n",
       "      <td>-0.573028</td>\n",
       "      <td>-0.260329</td>\n",
       "      <td>-0.846688</td>\n",
       "      <td>-0.756239</td>\n",
       "      <td>-0.552194</td>\n",
       "      <td>-0.271357</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.430717</td>\n",
       "      <td>-0.221643</td>\n",
       "      <td>-0.555861</td>\n",
       "      <td>-0.430277</td>\n",
       "      <td>-0.768683</td>\n",
       "      <td>-0.741829</td>\n",
       "      <td>-0.819078</td>\n",
       "      <td>-0.447176</td>\n",
       "      <td>-0.602122</td>\n",
       "      <td>-0.454962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>-0.013513</td>\n",
       "      <td>0.500586</td>\n",
       "      <td>0.458954</td>\n",
       "      <td>-0.355773</td>\n",
       "      <td>-0.775571</td>\n",
       "      <td>-0.047468</td>\n",
       "      <td>-0.703634</td>\n",
       "      <td>-0.666154</td>\n",
       "      <td>0.382991</td>\n",
       "      <td>0.040512</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.165825</td>\n",
       "      <td>0.159185</td>\n",
       "      <td>0.417704</td>\n",
       "      <td>-0.928206</td>\n",
       "      <td>0.103812</td>\n",
       "      <td>-0.587682</td>\n",
       "      <td>-0.872100</td>\n",
       "      <td>0.304056</td>\n",
       "      <td>-0.807301</td>\n",
       "      <td>-0.824883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>-0.671114</td>\n",
       "      <td>-0.785787</td>\n",
       "      <td>-0.455673</td>\n",
       "      <td>-0.635819</td>\n",
       "      <td>-0.454729</td>\n",
       "      <td>-0.871207</td>\n",
       "      <td>-0.799428</td>\n",
       "      <td>-0.811767</td>\n",
       "      <td>-0.918835</td>\n",
       "      <td>-0.781430</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.672283</td>\n",
       "      <td>-0.837840</td>\n",
       "      <td>-0.657070</td>\n",
       "      <td>-0.470545</td>\n",
       "      <td>-0.563013</td>\n",
       "      <td>-0.524826</td>\n",
       "      <td>-0.818779</td>\n",
       "      <td>-0.808618</td>\n",
       "      <td>-0.762099</td>\n",
       "      <td>-0.506521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>-0.725575</td>\n",
       "      <td>-0.735456</td>\n",
       "      <td>-0.603284</td>\n",
       "      <td>-0.889090</td>\n",
       "      <td>-0.753683</td>\n",
       "      <td>-0.629202</td>\n",
       "      <td>-0.699024</td>\n",
       "      <td>-0.881099</td>\n",
       "      <td>-0.681732</td>\n",
       "      <td>-0.806635</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.880290</td>\n",
       "      <td>-0.695576</td>\n",
       "      <td>-0.707545</td>\n",
       "      <td>-0.657497</td>\n",
       "      <td>-0.592146</td>\n",
       "      <td>-0.806301</td>\n",
       "      <td>-0.602686</td>\n",
       "      <td>-0.630750</td>\n",
       "      <td>-0.697590</td>\n",
       "      <td>-0.880033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>-0.154915</td>\n",
       "      <td>-0.862979</td>\n",
       "      <td>0.312859</td>\n",
       "      <td>-0.542639</td>\n",
       "      <td>0.223691</td>\n",
       "      <td>-0.707222</td>\n",
       "      <td>-0.154369</td>\n",
       "      <td>-0.079415</td>\n",
       "      <td>0.299417</td>\n",
       "      <td>0.078135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.379647</td>\n",
       "      <td>0.311974</td>\n",
       "      <td>0.263738</td>\n",
       "      <td>-0.389917</td>\n",
       "      <td>-0.399881</td>\n",
       "      <td>-0.546307</td>\n",
       "      <td>-0.319249</td>\n",
       "      <td>-0.453757</td>\n",
       "      <td>-0.650547</td>\n",
       "      <td>0.128386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>-0.408679</td>\n",
       "      <td>-0.242207</td>\n",
       "      <td>-0.287195</td>\n",
       "      <td>-0.800063</td>\n",
       "      <td>-0.325304</td>\n",
       "      <td>0.011674</td>\n",
       "      <td>-0.622707</td>\n",
       "      <td>-0.249473</td>\n",
       "      <td>-0.413118</td>\n",
       "      <td>-0.015836</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089841</td>\n",
       "      <td>-0.600761</td>\n",
       "      <td>-0.517343</td>\n",
       "      <td>0.016948</td>\n",
       "      <td>-0.235268</td>\n",
       "      <td>0.010273</td>\n",
       "      <td>-0.053873</td>\n",
       "      <td>-0.670436</td>\n",
       "      <td>-0.053040</td>\n",
       "      <td>-0.691091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>-0.297885</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.205010</td>\n",
       "      <td>-0.277650</td>\n",
       "      <td>-0.576150</td>\n",
       "      <td>-0.460585</td>\n",
       "      <td>-0.357701</td>\n",
       "      <td>0.244173</td>\n",
       "      <td>-0.354993</td>\n",
       "      <td>-0.840077</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.370737</td>\n",
       "      <td>-0.490045</td>\n",
       "      <td>-0.316701</td>\n",
       "      <td>-0.858814</td>\n",
       "      <td>0.114273</td>\n",
       "      <td>0.031259</td>\n",
       "      <td>0.182390</td>\n",
       "      <td>0.121687</td>\n",
       "      <td>0.232760</td>\n",
       "      <td>0.089192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>-0.007613</td>\n",
       "      <td>-0.583533</td>\n",
       "      <td>0.207330</td>\n",
       "      <td>-0.743402</td>\n",
       "      <td>0.024531</td>\n",
       "      <td>-0.765112</td>\n",
       "      <td>-0.205914</td>\n",
       "      <td>-0.339280</td>\n",
       "      <td>-0.301167</td>\n",
       "      <td>-0.397471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.543694</td>\n",
       "      <td>-0.332624</td>\n",
       "      <td>0.527367</td>\n",
       "      <td>0.255991</td>\n",
       "      <td>-0.690500</td>\n",
       "      <td>0.356867</td>\n",
       "      <td>0.216527</td>\n",
       "      <td>-0.140514</td>\n",
       "      <td>0.429551</td>\n",
       "      <td>0.441180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>-0.065543</td>\n",
       "      <td>0.221251</td>\n",
       "      <td>-0.819871</td>\n",
       "      <td>0.056111</td>\n",
       "      <td>-0.342204</td>\n",
       "      <td>0.144029</td>\n",
       "      <td>-0.001696</td>\n",
       "      <td>-0.017959</td>\n",
       "      <td>0.338366</td>\n",
       "      <td>-0.139479</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.404093</td>\n",
       "      <td>0.313786</td>\n",
       "      <td>-0.899589</td>\n",
       "      <td>-0.738782</td>\n",
       "      <td>0.359226</td>\n",
       "      <td>-0.006625</td>\n",
       "      <td>-0.127595</td>\n",
       "      <td>-0.048315</td>\n",
       "      <td>0.248053</td>\n",
       "      <td>0.263606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>-0.040889</td>\n",
       "      <td>0.212554</td>\n",
       "      <td>-0.778262</td>\n",
       "      <td>-0.248569</td>\n",
       "      <td>-0.806336</td>\n",
       "      <td>0.407578</td>\n",
       "      <td>0.485776</td>\n",
       "      <td>-0.403390</td>\n",
       "      <td>0.278174</td>\n",
       "      <td>0.471708</td>\n",
       "      <td>...</td>\n",
       "      <td>0.449050</td>\n",
       "      <td>0.056161</td>\n",
       "      <td>-0.679356</td>\n",
       "      <td>-0.332393</td>\n",
       "      <td>-0.363476</td>\n",
       "      <td>0.474998</td>\n",
       "      <td>-0.393078</td>\n",
       "      <td>-0.014247</td>\n",
       "      <td>0.526858</td>\n",
       "      <td>0.003058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>-0.101866</td>\n",
       "      <td>0.308571</td>\n",
       "      <td>-0.302045</td>\n",
       "      <td>0.425298</td>\n",
       "      <td>-0.324507</td>\n",
       "      <td>-0.361899</td>\n",
       "      <td>-0.801145</td>\n",
       "      <td>-0.738695</td>\n",
       "      <td>-0.177508</td>\n",
       "      <td>-0.439658</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.309343</td>\n",
       "      <td>-0.592920</td>\n",
       "      <td>0.126185</td>\n",
       "      <td>0.471569</td>\n",
       "      <td>-0.036835</td>\n",
       "      <td>0.014746</td>\n",
       "      <td>-0.112726</td>\n",
       "      <td>-0.627049</td>\n",
       "      <td>-0.886374</td>\n",
       "      <td>-0.542319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>-0.284085</td>\n",
       "      <td>-0.164462</td>\n",
       "      <td>-0.283740</td>\n",
       "      <td>-0.237367</td>\n",
       "      <td>0.235115</td>\n",
       "      <td>-0.121820</td>\n",
       "      <td>-0.767248</td>\n",
       "      <td>-0.683160</td>\n",
       "      <td>-0.634078</td>\n",
       "      <td>0.036146</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.285921</td>\n",
       "      <td>0.260346</td>\n",
       "      <td>-0.227140</td>\n",
       "      <td>0.034899</td>\n",
       "      <td>-0.183491</td>\n",
       "      <td>0.182726</td>\n",
       "      <td>0.041783</td>\n",
       "      <td>0.189041</td>\n",
       "      <td>0.135815</td>\n",
       "      <td>-0.770359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-0.576512</td>\n",
       "      <td>-0.689129</td>\n",
       "      <td>-0.316798</td>\n",
       "      <td>-0.462845</td>\n",
       "      <td>-0.467279</td>\n",
       "      <td>-0.516569</td>\n",
       "      <td>-0.834127</td>\n",
       "      <td>-0.523931</td>\n",
       "      <td>-0.449779</td>\n",
       "      <td>-0.517593</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.657396</td>\n",
       "      <td>-0.620024</td>\n",
       "      <td>-0.817439</td>\n",
       "      <td>-0.804879</td>\n",
       "      <td>-0.757389</td>\n",
       "      <td>-0.433786</td>\n",
       "      <td>-0.844563</td>\n",
       "      <td>-0.453617</td>\n",
       "      <td>-0.702751</td>\n",
       "      <td>-0.552199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-0.615478</td>\n",
       "      <td>-0.546227</td>\n",
       "      <td>-0.484576</td>\n",
       "      <td>-0.773919</td>\n",
       "      <td>-0.337226</td>\n",
       "      <td>-0.595903</td>\n",
       "      <td>-0.802397</td>\n",
       "      <td>-0.405686</td>\n",
       "      <td>-0.413153</td>\n",
       "      <td>-0.561133</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.653243</td>\n",
       "      <td>-0.755427</td>\n",
       "      <td>-0.380684</td>\n",
       "      <td>-0.414716</td>\n",
       "      <td>-0.374430</td>\n",
       "      <td>-0.688255</td>\n",
       "      <td>-0.555493</td>\n",
       "      <td>-0.542571</td>\n",
       "      <td>-0.621726</td>\n",
       "      <td>-0.471867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-0.040277</td>\n",
       "      <td>0.249176</td>\n",
       "      <td>0.201090</td>\n",
       "      <td>-0.622955</td>\n",
       "      <td>-0.001541</td>\n",
       "      <td>-0.017058</td>\n",
       "      <td>0.259912</td>\n",
       "      <td>-0.244803</td>\n",
       "      <td>-0.862780</td>\n",
       "      <td>-0.656290</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.893169</td>\n",
       "      <td>0.243036</td>\n",
       "      <td>-0.084305</td>\n",
       "      <td>0.464399</td>\n",
       "      <td>0.423555</td>\n",
       "      <td>0.338184</td>\n",
       "      <td>0.490716</td>\n",
       "      <td>0.204437</td>\n",
       "      <td>-0.824235</td>\n",
       "      <td>-0.813929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47 rows × 2020 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          H1        H2        H3        H4        H5        H6        H7  \\\n",
       "2  -0.146454  0.393924  0.339937 -0.013740 -0.682913 -0.862113  0.358242   \n",
       "9  -0.123105  0.342577 -0.565207 -0.638097 -0.647693  0.146720  0.090287   \n",
       "11 -0.369206 -0.257890  0.127110 -0.274984 -0.549932 -0.212014 -0.136864   \n",
       "12 -0.302154  0.069967  0.134479 -0.251923 -0.631796 -0.676151  0.007875   \n",
       "13 -0.054242 -0.640325  0.414762 -0.080123  0.452996  0.409956  0.286226   \n",
       "14 -0.619068 -0.750709 -0.452578 -0.535464 -0.602328 -0.436257 -0.912040   \n",
       "15 -0.108487 -0.813365 -0.569529 -0.636228 -0.594005  0.037724 -0.543479   \n",
       "20 -0.360619 -0.729156 -0.901321 -0.812348 -0.703690 -0.853144 -0.598033   \n",
       "21 -0.026694 -0.709150 -0.461017 -0.650008 -0.393988  0.175869  0.502333   \n",
       "22 -0.149954  0.218512 -0.247831 -0.611182  0.310886 -0.131841 -0.720729   \n",
       "26 -0.320377 -0.027639 -0.228123 -0.193938 -0.899995  0.147507 -0.648720   \n",
       "35 -0.727864 -0.777697 -0.768861 -0.766478 -0.625764 -0.676353 -0.659890   \n",
       "37 -0.173121  0.125609  0.292662 -0.682206 -0.205392 -0.002684 -0.261327   \n",
       "38 -0.137399 -0.641446 -0.731599 -0.177338 -0.513616 -0.752027 -0.491141   \n",
       "40 -0.115728 -0.612697 -0.647319  0.043518 -0.407839 -0.149138  0.303518   \n",
       "41 -0.239078  0.255608  0.266345 -0.774457 -0.384734 -0.298178  0.212852   \n",
       "42 -0.192391 -0.264184  0.352185  0.240108  0.188163  0.146459  0.292668   \n",
       "43 -0.791127 -0.783998 -0.760333 -0.771955 -0.872342 -0.884482 -0.717253   \n",
       "46 -0.069726  0.480122 -0.563680 -0.658898  0.254001 -0.066413 -0.206361   \n",
       "47 -0.174198 -0.552304 -0.771860  0.276304 -0.265590  0.227499 -0.064400   \n",
       "53 -0.164791 -0.570953 -0.595590 -0.681665 -0.563707  0.295036 -0.275353   \n",
       "55 -0.271016 -0.752087  0.268088  0.287097 -0.841718 -0.551340 -0.316245   \n",
       "56 -0.575777 -0.526521 -0.297284 -0.587791 -0.697098 -0.615715 -0.647686   \n",
       "58 -0.458432 -0.160996 -0.046515 -0.310189 -0.641977 -0.034673 -0.625834   \n",
       "59 -0.100074 -0.624728  0.352491  0.202085  0.238233 -0.837509 -0.266839   \n",
       "60 -0.677994 -0.769293 -0.778466 -0.709285 -0.658713 -0.654868 -0.511571   \n",
       "62 -0.726053 -0.885843 -0.580725 -0.792799 -0.661731 -0.654271 -0.640560   \n",
       "63 -0.405918 -0.332246 -0.201748 -0.522411 -0.608318  0.043507 -0.011711   \n",
       "64 -0.702962 -0.629039 -0.853000 -0.521506 -0.793314 -0.619118 -0.624551   \n",
       "65 -0.085139 -0.437690 -0.166883 -0.587736 -0.807592  0.286397  0.369632   \n",
       "67 -0.012403 -0.799444 -0.671281 -0.715589  0.135307 -0.141979  0.258032   \n",
       "68 -0.412675 -0.529937 -0.738500 -0.702838 -0.406765  0.002707 -0.838964   \n",
       "69 -0.531818 -0.200855 -0.677934 -0.649973 -0.573028 -0.260329 -0.846688   \n",
       "73 -0.013513  0.500586  0.458954 -0.355773 -0.775571 -0.047468 -0.703634   \n",
       "74 -0.671114 -0.785787 -0.455673 -0.635819 -0.454729 -0.871207 -0.799428   \n",
       "75 -0.725575 -0.735456 -0.603284 -0.889090 -0.753683 -0.629202 -0.699024   \n",
       "78 -0.154915 -0.862979  0.312859 -0.542639  0.223691 -0.707222 -0.154369   \n",
       "79 -0.408679 -0.242207 -0.287195 -0.800063 -0.325304  0.011674 -0.622707   \n",
       "80 -0.297885  0.033898  0.205010 -0.277650 -0.576150 -0.460585 -0.357701   \n",
       "81 -0.007613 -0.583533  0.207330 -0.743402  0.024531 -0.765112 -0.205914   \n",
       "82 -0.065543  0.221251 -0.819871  0.056111 -0.342204  0.144029 -0.001696   \n",
       "85 -0.040889  0.212554 -0.778262 -0.248569 -0.806336  0.407578  0.485776   \n",
       "88 -0.101866  0.308571 -0.302045  0.425298 -0.324507 -0.361899 -0.801145   \n",
       "93 -0.284085 -0.164462 -0.283740 -0.237367  0.235115 -0.121820 -0.767248   \n",
       "95 -0.576512 -0.689129 -0.316798 -0.462845 -0.467279 -0.516569 -0.834127   \n",
       "97 -0.615478 -0.546227 -0.484576 -0.773919 -0.337226 -0.595903 -0.802397   \n",
       "98 -0.040277  0.249176  0.201090 -0.622955 -0.001541 -0.017058  0.259912   \n",
       "\n",
       "          H8        H9       H10    ...        R1991     R1992     R1993  \\\n",
       "2  -0.403218  0.343756  0.032275    ...    -0.577826 -0.821057 -0.614194   \n",
       "9  -0.554072 -0.629386  0.103788    ...     0.267269 -0.914097 -0.267564   \n",
       "11 -0.240905 -0.226033 -0.778029    ...    -0.495367 -0.668827 -0.258218   \n",
       "12  0.221255 -0.342874 -0.312439    ...    -0.424961  0.234883 -0.670412   \n",
       "13  0.311934  0.290435 -0.535879    ...     0.414786 -0.754571 -0.073655   \n",
       "14 -0.865021 -0.825729 -0.672619    ...    -0.468339 -0.811633 -0.348598   \n",
       "15 -0.492653 -0.171989  0.155846    ...    -0.580126 -0.774531 -0.408017   \n",
       "20 -0.608235 -0.127305 -0.060557    ...    -0.530709 -0.503198 -0.795508   \n",
       "21 -0.278094  0.345569 -0.441543    ...    -0.713565 -0.232869 -0.021737   \n",
       "22 -0.604925  0.121482  0.357922    ...     0.119370  0.322688 -0.497251   \n",
       "26  0.108155 -0.032898 -0.833387    ...     0.197516  0.117502 -0.008177   \n",
       "35 -0.871844 -0.753360 -0.860117    ...    -0.768097 -0.848651 -0.704718   \n",
       "37  0.179838 -0.556664  0.251159    ...     0.097758 -0.683447 -0.778245   \n",
       "38 -0.273972 -0.782325  0.232018    ...     0.193190  0.014305  0.343534   \n",
       "40 -0.577919  0.369182 -0.501608    ...     0.401756  0.176516  0.429257   \n",
       "41 -0.047491  0.241916  0.241586    ...    -0.485698 -0.212894 -0.143978   \n",
       "42  0.121619  0.322182  0.322255    ...    -0.426058 -0.517130  0.364654   \n",
       "43 -0.766407 -0.853865 -0.881081    ...    -0.803882 -0.887366 -0.832764   \n",
       "46 -0.184525  0.481877 -0.046445    ...    -0.123788 -0.709103 -0.522550   \n",
       "47 -0.577656 -0.868770 -0.474837    ...    -0.847679  0.019733  0.019234   \n",
       "53 -0.244861 -0.108978  0.350997    ...    -0.644723 -0.015753 -0.368660   \n",
       "55 -0.931633 -0.736283 -0.126570    ...    -0.656337 -0.482672 -0.375802   \n",
       "56 -0.799267 -0.611824 -0.258940    ...    -0.595525 -0.510200 -0.656619   \n",
       "58 -0.438954 -0.756202 -0.724667    ...    -0.776682 -0.232737 -0.774176   \n",
       "59  0.271150 -0.014574  0.190013    ...     0.197669 -0.747980 -0.251005   \n",
       "60 -0.553220 -0.700434 -0.804784    ...    -0.728787 -0.670887 -0.828161   \n",
       "62 -0.789205 -0.812468 -0.671632    ...    -0.824009 -0.587760 -0.613126   \n",
       "63 -0.245873 -0.572549 -0.549952    ...    -0.270272 -0.138870 -0.428834   \n",
       "64 -0.713820 -0.574140 -0.604150    ...    -0.850071 -0.876611 -0.588366   \n",
       "65  0.238438  0.348872  0.050215    ...     0.280573 -0.805946 -0.090876   \n",
       "67 -0.665683 -0.890065 -0.545057    ...    -0.175800 -0.559352 -0.010627   \n",
       "68 -0.770073 -0.474638 -0.821904    ...    -0.551023 -0.088619  0.063729   \n",
       "69 -0.756239 -0.552194 -0.271357    ...    -0.430717 -0.221643 -0.555861   \n",
       "73 -0.666154  0.382991  0.040512    ...    -0.165825  0.159185  0.417704   \n",
       "74 -0.811767 -0.918835 -0.781430    ...    -0.672283 -0.837840 -0.657070   \n",
       "75 -0.881099 -0.681732 -0.806635    ...    -0.880290 -0.695576 -0.707545   \n",
       "78 -0.079415  0.299417  0.078135    ...     0.379647  0.311974  0.263738   \n",
       "79 -0.249473 -0.413118 -0.015836    ...     0.089841 -0.600761 -0.517343   \n",
       "80  0.244173 -0.354993 -0.840077    ...    -0.370737 -0.490045 -0.316701   \n",
       "81 -0.339280 -0.301167 -0.397471    ...     0.543694 -0.332624  0.527367   \n",
       "82 -0.017959  0.338366 -0.139479    ...    -0.404093  0.313786 -0.899589   \n",
       "85 -0.403390  0.278174  0.471708    ...     0.449050  0.056161 -0.679356   \n",
       "88 -0.738695 -0.177508 -0.439658    ...    -0.309343 -0.592920  0.126185   \n",
       "93 -0.683160 -0.634078  0.036146    ...    -0.285921  0.260346 -0.227140   \n",
       "95 -0.523931 -0.449779 -0.517593    ...    -0.657396 -0.620024 -0.817439   \n",
       "97 -0.405686 -0.413153 -0.561133    ...    -0.653243 -0.755427 -0.380684   \n",
       "98 -0.244803 -0.862780 -0.656290    ...    -0.893169  0.243036 -0.084305   \n",
       "\n",
       "       R1994     R1995     R1996     R1997     R1998     R1999     R2000  \n",
       "2  -0.514526  0.011969 -0.513696  0.231489 -0.463622  0.286866 -0.401187  \n",
       "9   0.313976 -0.697473  0.100308 -0.586745  0.241890  0.076020  0.117782  \n",
       "11 -0.762129  0.034462 -0.103812  0.030413 -0.766085 -0.552753 -0.733558  \n",
       "12  0.040422  0.052441 -0.761853 -0.051239 -0.264500 -0.157644 -0.519173  \n",
       "13 -0.358876  0.297296  0.129629 -0.168108  0.259798 -0.434886 -0.220696  \n",
       "14 -0.371508 -0.685192 -0.378291 -0.452907 -0.812458 -0.655101 -0.498714  \n",
       "15  0.333990 -0.638109 -0.563338 -0.600042  0.432799 -0.448603  0.350062  \n",
       "20 -0.315068 -0.210174 -0.676759  0.036025 -0.830868 -0.442693 -0.017176  \n",
       "21 -0.033586  0.481271  0.218786 -0.258029  0.276492  0.417444 -0.351808  \n",
       "22  0.072150 -0.908411 -0.585910 -0.270094  0.350221 -0.346046 -0.455285  \n",
       "26 -0.718462  0.021826  0.110930 -0.088941 -0.820045  0.083175  0.130579  \n",
       "35 -0.675440 -0.751530 -0.669866 -0.827903 -0.578824 -0.733760 -0.837847  \n",
       "37 -0.568389 -0.688306 -0.116439 -0.922407 -0.838570  0.099159 -0.499037  \n",
       "38  0.420829 -0.600151  0.223641 -0.687182  0.285739 -0.585647  0.263197  \n",
       "40 -0.307652 -0.027169  0.030720 -0.682900  0.335323 -0.004529  0.206348  \n",
       "41 -0.702152  0.040977 -0.797091  0.175576 -0.791013 -0.236242 -0.597688  \n",
       "42  0.096322 -0.697453  0.378244  0.100149  0.360793  0.315504  0.313568  \n",
       "43 -0.736727 -0.723952 -0.936283 -0.883473 -0.837366 -0.723727 -0.924070  \n",
       "46  0.490898  0.134941  0.464815  0.082457 -0.011648 -0.348777 -0.847112  \n",
       "47 -0.934142  0.357022  0.152928  0.291266  0.079366 -0.719548 -0.585896  \n",
       "53 -0.272713 -0.602167 -0.509827 -0.689267 -0.333169 -0.417147  0.343354  \n",
       "55 -0.418051  0.068873 -0.600823  0.169611 -0.056169 -0.104672 -0.223484  \n",
       "56 -0.833475 -0.447636 -0.288656 -0.573503 -0.235741 -0.823067 -0.260389  \n",
       "58 -0.746069 -0.075718 -0.672116 -0.249068 -0.350647 -0.267807 -0.589213  \n",
       "59 -0.076150 -0.151520  0.309565 -0.426784 -0.571788 -0.549621 -0.068640  \n",
       "60 -0.761420 -0.716881 -0.714818 -0.708997 -0.772406 -0.670170 -0.483938  \n",
       "62 -0.821629 -0.598267 -0.609580 -0.679488 -0.891103 -0.818379 -0.635315  \n",
       "63  0.097650 -0.011277 -0.811374 -0.087922 -0.176553 -0.066252 -0.578392  \n",
       "64 -0.780267 -0.876126 -0.562931 -0.767505 -0.718746 -0.919576 -0.572401  \n",
       "65 -0.477314 -0.686449  0.449345 -0.603528 -0.847297 -0.407975 -0.558467  \n",
       "67  0.505123  0.506937 -0.187398 -0.535700 -0.117415 -0.035468 -0.698877  \n",
       "68 -0.872245  0.065849 -0.637168 -0.670604 -0.786506 -0.709769 -0.398859  \n",
       "69 -0.430277 -0.768683 -0.741829 -0.819078 -0.447176 -0.602122 -0.454962  \n",
       "73 -0.928206  0.103812 -0.587682 -0.872100  0.304056 -0.807301 -0.824883  \n",
       "74 -0.470545 -0.563013 -0.524826 -0.818779 -0.808618 -0.762099 -0.506521  \n",
       "75 -0.657497 -0.592146 -0.806301 -0.602686 -0.630750 -0.697590 -0.880033  \n",
       "78 -0.389917 -0.399881 -0.546307 -0.319249 -0.453757 -0.650547  0.128386  \n",
       "79  0.016948 -0.235268  0.010273 -0.053873 -0.670436 -0.053040 -0.691091  \n",
       "80 -0.858814  0.114273  0.031259  0.182390  0.121687  0.232760  0.089192  \n",
       "81  0.255991 -0.690500  0.356867  0.216527 -0.140514  0.429551  0.441180  \n",
       "82 -0.738782  0.359226 -0.006625 -0.127595 -0.048315  0.248053  0.263606  \n",
       "85 -0.332393 -0.363476  0.474998 -0.393078 -0.014247  0.526858  0.003058  \n",
       "88  0.471569 -0.036835  0.014746 -0.112726 -0.627049 -0.886374 -0.542319  \n",
       "93  0.034899 -0.183491  0.182726  0.041783  0.189041  0.135815 -0.770359  \n",
       "95 -0.804879 -0.757389 -0.433786 -0.844563 -0.453617 -0.702751 -0.552199  \n",
       "97 -0.414716 -0.374430 -0.688255 -0.555493 -0.542571 -0.621726 -0.471867  \n",
       "98  0.464399  0.423555  0.338184  0.490716  0.204437 -0.824235 -0.813929  \n",
       "\n",
       "[47 rows x 2020 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_reward=[]\n",
    "for i in range(0,20):\n",
    "    column_reward.append(historical_reward[i])\n",
    "for i in range(0,no_of_trials):\n",
    "    column_reward.append(present_reward[i])\n",
    "\n",
    "dataset_reward = pd.DataFrame({})\n",
    "\n",
    "dataset_reward['H1']=column_reward[0]\n",
    "for j in range (0,19):\n",
    "    dataset_reward[h[j]]=column_reward[j+1]\n",
    "    \n",
    "for j in range(19,no_of_trials+19):\n",
    "    dataset_reward[r[j-19]]=column_reward[j+1]\n",
    "    \n",
    "dataset_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#historical reward vector has different indexes \n",
    "#hence to take average columnwise, we have to construsct another variable 'p'\n",
    "p=[]\n",
    "for i in range(0,20):\n",
    "    p.append(historical_reward[i][k[1]])\n",
    "#p\n",
    "#sum(p)/len(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.2165236549686489, -0.13720847699043737, -0.3297393723556752, -0.40115968367619254, 0.11337780386991267, -0.6388000487800473, -0.22032143896338724, -0.42121522589874827, -0.1489095640204575, -0.23371561694131024, -0.3284785406666406, -0.7569265258286515, -0.16102142684669724, -0.3501467880758594, -0.17654408986737363, -0.2365863003248431, 0.03775950603888005, -0.8109740053766317, -0.046677147168457375, -0.25763574359291946, -0.23138909481347167, -0.25783563041297675, -0.5344741669707368, -0.47833113575834363, -0.1262319206667482, -0.628302377436083, -0.7053924711951964, -0.33787649473324144, -0.699852449507563, -0.06719861691012866, -0.4038000729169034, -0.48008477833308894, -0.5631477083493149, -0.042231461329834165, -0.6904497946686586, -0.695606585789918, -0.22752429807667926, -0.33128765903340274, -0.2286251893752734, -0.18713709971073222, -0.14472449713442975, -0.03319407020643358, -0.1959719751910208, -0.39859718415504364, -0.5240891226229114, -0.5696862455980554, -0.13387819679707072]\n"
     ]
    }
   ],
   "source": [
    "#get estimated reward vector at trial t\n",
    "def estimated_reward(t):#get_mean()\n",
    "    asset, trial = no_of_assets, no_of_trials;\n",
    "    r_hat = [] #\\hat{r_{i,t}}\n",
    "    for i in range(0,len(k)):\n",
    "        if(t==1):\n",
    "            p=[]\n",
    "            for j in range(0,20):\n",
    "                p.append(historical_reward[j][k[i]])\n",
    "            r_hat.append(sum(p)/len(p))\n",
    "            #r_hat.append(df.loc[i,\"H1\":\"H20\"].mean()) \n",
    "        else:\n",
    "            q=[]\n",
    "            for j in range(0,20):\n",
    "                q.append(historical_reward[j][k[i]])\n",
    "            for j in range(0,t-1):#trial 2 par R1 tak ka mean chahiye which is present_reward[0]\n",
    "                q.append(present_reward[j][k[i]])\n",
    "            r_hat.append(sum(q)/len(q))\n",
    "            #r_hat.append(df.loc[i,\"H1\":\"R\"+str(t-1)].mean())\n",
    "    return r_hat\n",
    "#print(estimated_reward(1))\n",
    "print(estimated_reward(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03554978688431362, 0.043765391093897904, -0.14876550427133992, -0.22018581559185726, 0.29435167195424794, -0.457826180695712, -0.03934757087905197, -0.240241357814413, 0.03206430406387778, -0.05274174885697497, -0.14750467258230535, -0.5759526577443163, 0.019952441237638036, -0.16917291999152415, 0.004429778216961638, -0.055612432240507825, 0.2187333741232153, -0.6300001372922964, 0.1342967209158779, -0.07666187550858419, -0.0504152267291364, -0.07686176232864148, -0.35350029888640155, -0.2973572676740084, 0.05474194741758706, -0.4473285093517477, -0.5244186031108612, -0.15690262664890617, -0.5188785814232277, 0.11377525117420662, -0.22282620483256813, -0.2991109102487537, -0.3821738402649797, 0.1387424067545011, -0.5094759265843234, -0.5146327177055827, -0.04655042999234399, -0.15031379094906747, -0.04765132129093813, -0.006163231626396953, 0.03624937094990552, 0.1477797978779017, -0.014998107106685532, -0.21762331607070837, -0.34311525453857616, -0.3887123775137201, 0.04709567128726455]\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "def estimated_reward_plus(t):\n",
    "    #n=number_of_assets in formula\n",
    "    #vector=np.array([20] * df.shape[0])\n",
    "        \n",
    "    T=[None]*(no_of_assets+1)\n",
    "    for i in range(0,len(k)):\n",
    "        T[k[i]]=20\n",
    "    for j in range(0,t):\n",
    "        for i in range(0,len(k)):\n",
    "            T[k[i]]=T[k[i]]+1\n",
    "\n",
    "    error=[(1/(2*T[i]))*np.log((2*no_of_assets)/(mu)) for i in (k)]\n",
    "    ret=map(sum, zip(estimated_reward(t),error))   \n",
    "    return list(ret)\n",
    "    \n",
    "def reward_plus(t):\n",
    "    #n=number_of_assets in formula\n",
    "    #vector=np.array([20] * df.shape[0])\n",
    "        \n",
    "    T=[None]*(no_of_assets+1)\n",
    "    for i in range(0,len(k)):\n",
    "        T[k[i]]=20\n",
    "    for j in range(0,t):\n",
    "        for i in range(0,len(k)):\n",
    "            T[k[i]]=T[k[i]]+1\n",
    "\n",
    "    error=[(1/(2*T[i]))*np.log((2*no_of_assets)/(mu)) for i in (k)]\n",
    "    #ret=map(sum, zip(estimated_reward(t),error))   \n",
    "    return list(error)\n",
    "    \n",
    "def estimated_reward_minus(t):\n",
    "    T=[None]*(no_of_assets+1)\n",
    "    for i in range(0,len(k)):\n",
    "        T[k[i]]=20\n",
    "    for j in range(0,t):\n",
    "        for i in range(0,len(k)):\n",
    "            T[k[i]]=T[k[i]]+1\n",
    "\n",
    "    error=[(1/(2*T[i]))*np.log((2*no_of_assets)/(mu)) for i in (k)]\n",
    "    ret=map(operator.sub, estimated_reward(t),error)\n",
    "        \n",
    "    return list(ret)\n",
    "def reward_minus(t):\n",
    "    T=[None]*(no_of_assets+1)\n",
    "    for i in range(0,len(k)):\n",
    "        T[k[i]]=20\n",
    "    for j in range(0,t):\n",
    "        for i in range(0,len(k)):\n",
    "            T[k[i]]=T[k[i]]+1\n",
    "\n",
    "    error=[-((1/(2*T[i]))*np.log((2*no_of_assets)/(mu))) for i in (k)]\n",
    "    \n",
    "        \n",
    "    return list(error)\n",
    "    \n",
    "print(estimated_reward_plus(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.39749752305298414, -0.3181823450747726, -0.5107132404400104, -0.5821335517605278, -0.0675960642144226, -0.8197739168643825, -0.40129530704772254, -0.6021890939830835, -0.32988343210479276, -0.4146894850256455, -0.5094524087509759, -0.9379003939129867, -0.34199529493103253, -0.5311206561601947, -0.35751795795170893, -0.41756016840917837, -0.14321436204545523, -0.9919478734609669, -0.22765101525279263, -0.43860961167725476, -0.41236296289780694, -0.438809498497312, -0.715448035055072, -0.6593050038426789, -0.3072057887510835, -0.8092762455204182, -0.8863663392795317, -0.5188503628175767, -0.8808263175918982, -0.24817248499446393, -0.5847739410012387, -0.6610586464174242, -0.7441215764336502, -0.22320532941416943, -0.8714236627529939, -0.8765804538742532, -0.40849816616101453, -0.512261527117738, -0.4095990574596087, -0.36811096779506747, -0.32569836521876505, -0.21416793829076886, -0.3769458432753561, -0.5795710522393789, -0.7050629907072467, -0.7506601136823906, -0.314852064881406]\n"
     ]
    }
   ],
   "source": [
    "print(estimated_reward_minus(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#appending element to a list\n",
    "#n=get_cost(1)\n",
    "#n.append(1)\n",
    "#n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(get_cost(1))\n",
    "#print(k)\n",
    "#test code to construct (get_cost_for_k(t))\n",
    "#m=[get_cost(1)[x] for x in k]# here if k[0]=2, then it will give get_cost(1)[2], i.e. 3rd entry, hence not correct\n",
    "#m=[get_cost(2)[x-1] for x in k]\n",
    "#m\n",
    "\n",
    "#get_cost_for_k\n",
    "def get_cost_for_k(t):\n",
    "    m=[get_cost(t)[x-1] for x in k]\n",
    "    return m\n",
    "#get_cost_for_k(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class BaseOptimizer:\n",
    "    def __init__(self, n_assets, tickers=None):\n",
    "        \"\"\"\n",
    "        :param n_assets: number of assets\n",
    "        :type n_assets: int\n",
    "        :param tickers: name of assets\n",
    "        :type tickers: list\n",
    "        \"\"\"\n",
    "        self.n_assets = n_assets\n",
    "        if tickers is None:\n",
    "            self.tickers = list(range(n_assets))\n",
    "        else:\n",
    "            self.tickers = tickers\n",
    "        # Outputs\n",
    "        self.weights = None\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        if self.weights is None:\n",
    "            self.weights = [0] * self.n_assets\n",
    "        for i, k in enumerate(self.tickers):\n",
    "            if k in weights:\n",
    "                self.weights[i] = weights[k]\n",
    "\n",
    "    def clean_weights(self, cutoff=1e-4, rounding=5):\n",
    "        \"\"\"\n",
    "        Helper method to clean the raw weights, setting any weights whose absolute\n",
    "        values are below the cutoff to zero, and rounding the rest.\n",
    "        :param cutoff: the lower bound, defaults to 1e-4\n",
    "        :type cutoff: float, optional\n",
    "        :param rounding: number of decimal places to round the weights, defaults to 5.\n",
    "                         Set to None if rounding is not desired.\n",
    "        :type rounding: int, optional\n",
    "        :return: asset weights\n",
    "        :rtype: dict\n",
    "        \"\"\"\n",
    "        if not isinstance(rounding, int) or rounding < 1:\n",
    "            raise ValueError(\"rounding must be a positive integer\")\n",
    "        clean_weights = self.weights.copy()\n",
    "        clean_weights[np.abs(clean_weights) < cutoff] = 0\n",
    "        if rounding is not None:\n",
    "            clean_weights = np.round(clean_weights, rounding)\n",
    "        return dict(zip(self.tickers, clean_weights))\n",
    "\n",
    "\n",
    "class BaseScipyOptimizer(BaseOptimizer):\n",
    "    def __init__(self, n_assets, tickers=None, weight_bounds=(0, 1)):\n",
    "        \"\"\"\n",
    "        :param weight_bounds: minimum and maximum weight of an asset, defaults to (0, 1).\n",
    "                              Must be changed to (-1, 1) for portfolios with shorting.\n",
    "        :type weight_bounds: tuple, optional\n",
    "        \"\"\"\n",
    "        super().__init__(n_assets, tickers)\n",
    "        self.bounds = self._make_valid_bounds(weight_bounds)\n",
    "        # Optimisation parameters\n",
    "        self.initial_guess = np.array([1 / self.n_assets] * self.n_assets)\n",
    "        self.constraints = [{\"type\": \"eq\", \"fun\": lambda x: np.sum(x) - 1}]\n",
    "\n",
    "    def _make_valid_bounds(self, test_bounds):\n",
    "        \"\"\"\n",
    "        Private method: process input bounds into a form acceptable by scipy.optimize,\n",
    "        and check the validity of said bounds.\n",
    "        :param test_bounds: minimum and maximum weight of an asset\n",
    "        :type test_bounds: tuple\n",
    "        :raises ValueError: if ``test_bounds`` is not a tuple of length two.\n",
    "        :raises ValueError: if the lower bound is too high\n",
    "        :return: a tuple of bounds, e.g ((0, 1), (0, 1), (0, 1) ...)\n",
    "        :rtype: tuple of tuples\n",
    "        \"\"\"\n",
    "        if len(test_bounds) != 2 or not isinstance(test_bounds, tuple):\n",
    "            raise ValueError(\n",
    "                \"test_bounds must be a tuple of (lower bound, upper bound)\"\n",
    "            )\n",
    "        if test_bounds[0] is not None:\n",
    "            if test_bounds[0] * self.n_assets > 1:\n",
    "                raise ValueError(\"Lower bound is too high\")\n",
    "        return (test_bounds,) * self.n_assets\n",
    "\n",
    "\n",
    "def portfolio_performance(\n",
    "    expected_returns, cov_matrix, weights, verbose=False, risk_free_rate=0.02\n",
    "):\n",
    "    \"\"\"\n",
    "    After optimising, calculate (and optionally print) the performance of the optimal\n",
    "    portfolio. Currently calculates expected return, volatility, and the Sharpe ratio.\n",
    "    :param expected_returns: expected returns for each asset. Set to None if\n",
    "                             optimising for volatility only.\n",
    "    :type expected_returns: pd.Series, list, np.ndarray\n",
    "    :param cov_matrix: covariance of returns for each asset\n",
    "    :type cov_matrix: pd.DataFrame or np.array\n",
    "    :param weights: weights or assets\n",
    "    :type weights: list, np.array or dict, optional\n",
    "    :param verbose: whether performance should be printed, defaults to False\n",
    "    :type verbose: bool, optional\n",
    "    :param risk_free_rate: risk-free rate of borrowing/lending, defaults to 0.02\n",
    "    :type risk_free_rate: float, optional\n",
    "    :raises ValueError: if weights have not been calcualted yet\n",
    "    :return: expected return, volatility, Sharpe ratio.\n",
    "    :rtype: (float, float, float)\n",
    "    \"\"\"\n",
    "    if isinstance(weights, dict):\n",
    "        if isinstance(expected_returns, pd.Series):\n",
    "            tickers = list(expected_returns.index)\n",
    "        elif isinstance(cov_matrix, pd.DataFrame):\n",
    "            tickers = list(cov_matrix.columns)\n",
    "        else:\n",
    "            tickers = list(range(len(expected_returns)))\n",
    "        newweights = np.zeros(len(tickers))\n",
    "        for i, k in enumerate(tickers):\n",
    "            if k in weights:\n",
    "                newweights[i] = weights[k]\n",
    "        if newweights.sum() == 0:\n",
    "            raise ValueError(\"Weights add to zero, or ticker names don't match\")\n",
    "    elif weights is not None:\n",
    "        newweights = np.asarray(weights)\n",
    "    else:\n",
    "        raise ValueError(\"Weights is None\")\n",
    "    sigma = np.sqrt(objective_functions.volatility(newweights, cov_matrix))\n",
    "    mu = newweights.dot(expected_returns)\n",
    "\n",
    "    sharpe = -objective_functions.negative_sharpe(\n",
    "        newweights, expected_returns, cov_matrix, risk_free_rate = risk_free_rate\n",
    "    )\n",
    "    if verbose:\n",
    "        print(\"Expected annual return: {:.1f}%\".format(100 * mu))\n",
    "        print(\"Annual volatility: {:.1f}%\".format(100 * sigma))\n",
    "        print(\"Sharpe Ratio: {:.2f}\".format(sharpe))\n",
    "    return mu, sigma, sharpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import noisyopt\n",
    "\n",
    "\n",
    "class CVAROpt(BaseScipyOptimizer):\n",
    "\n",
    "    \"\"\"\n",
    "    A CVAROpt object (inheriting from BaseScipyOptimizer) provides a method for\n",
    "    optimising the CVaR (a.k.a expected shortfall) of a portfolio.\n",
    "    Instance variables:\n",
    "    - Inputs\n",
    "        - ``tickers``\n",
    "        - ``returns``\n",
    "        - ``bounds``\n",
    "    - Optimisation parameters:\n",
    "        - ``s``: the number of Monte Carlo simulations\n",
    "        - ``beta``: the critical value\n",
    "    - Output: ``weights``\n",
    "    Public methods:\n",
    "    - ``min_cvar()``\n",
    "    - ``normalize_weights()``\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, returns, weight_bounds=(0, 1)):\n",
    "        \"\"\"\n",
    "        :param returns: asset historical returns\n",
    "        :type returns: pd.DataFrame\n",
    "        :param weight_bounds: minimum and maximum weight of an asset, defaults to (0, 1).\n",
    "                              Must be changed to (-1, 1) for portfolios with shorting.\n",
    "                              For CVaR opt, this is not a hard boundary.\n",
    "        :type weight_bounds: tuple, optional\n",
    "        :raises TypeError: if ``returns`` is not a dataframe\n",
    "        \"\"\"\n",
    "        if not isinstance(returns, pd.DataFrame):\n",
    "            raise TypeError(\"returns are not a dataframe\")\n",
    "        self.returns = returns\n",
    "        tickers = returns.columns\n",
    "        super().__init__(len(tickers), tickers, weight_bounds)\n",
    "\n",
    "    def min_cvar(self, s=10000, beta=0.95, random_state=None):\n",
    "        \"\"\"\n",
    "        Find the portfolio weights that minimises the CVaR, via\n",
    "        Monte Carlo sampling from the return distribution.\n",
    "        :param s: number of bootstrap draws, defaults to 10000\n",
    "        :type s: int, optional\n",
    "        :param beta: \"significance level\" (i. 1 - q), defaults to 0.95\n",
    "        :type beta: float, optional\n",
    "        :param random_state: seed for random sampling, defaults to None\n",
    "        :type random_state: int, optional\n",
    "        :return: asset weights for the Sharpe-maximising portfolio\n",
    "        :rtype: dict\n",
    "        \"\"\"\n",
    "        args = (self.returns, s, beta, random_state)\n",
    "        result = noisyopt.minimizeSPSA(\n",
    "            objective_functions.negative_cvar,\n",
    "            args=args,\n",
    "            bounds=self.bounds,\n",
    "            x0=self.initial_guess,\n",
    "            niter=1000,\n",
    "            paired=False,\n",
    "        )\n",
    "        self.weights = self.normalize_weights(result[\"x\"])\n",
    "        return dict(zip(self.tickers, self.weights))\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_weights(raw_weights):\n",
    "        \"\"\"\n",
    "        Make all weights sum to 1\n",
    "        :param raw_weights: input weights which do not sum to 1\n",
    "        :type raw_weights: np.array, pd.Series\n",
    "        :return: normalized weights\n",
    "        :rtype: np.array, pd.Series\n",
    "        \"\"\"\n",
    "        return raw_weights / raw_weights.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_mean_return(weights, expected_returns):\n",
    "    \"\"\"\n",
    "    Calculate the negative mean return of a portfolio\n",
    "    :param weights: asset weights of the portfolio\n",
    "    :type weights: np.ndarray\n",
    "    :param expected_returns: expected return of each asset\n",
    "    :type expected_returns: pd.Series\n",
    "    :return: negative mean return\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    return -weights.dot(expected_returns)\n",
    "\n",
    "\n",
    "def negative_sharpe(\n",
    "    weights, expected_returns, cov_matrix, gamma=0, risk_free_rate=0.02\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the negative Sharpe ratio of a portfolio\n",
    "    :param weights: asset weights of the portfolio\n",
    "    :type weights: np.ndarray\n",
    "    :param expected_returns: expected return of each asset\n",
    "    :type expected_returns: pd.Series\n",
    "    :param cov_matrix: the covariance matrix of asset returns\n",
    "    :type cov_matrix: pd.DataFrame\n",
    "    :param gamma: L2 regularisation parameter, defaults to 0. Increase if you want more\n",
    "                    non-negligible weights\n",
    "    :type gamma: float, optional\n",
    "    :param risk_free_rate: risk-free rate of borrowing/lending, defaults to 0.02\n",
    "    :type risk_free_rate: float, optional\n",
    "    :return: negative Sharpe ratio\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    mu = weights.dot(expected_returns)\n",
    "    sigma = np.sqrt(np.dot(weights, np.dot(cov_matrix, weights.T)))\n",
    "    L2_reg = gamma * (weights ** 2).sum()\n",
    "    return -(mu - risk_free_rate) / sigma + L2_reg\n",
    "\n",
    "\n",
    "def volatility(weights, cov_matrix, gamma=0):\n",
    "    \"\"\"\n",
    "    Calculate the volatility of a portfolio. This is actually a misnomer because\n",
    "    the function returns variance, which is technically the correct objective\n",
    "    function when minimising volatility.\n",
    "    :param weights: asset weights of the portfolio\n",
    "    :type weights: np.ndarray\n",
    "    :param cov_matrix: the covariance matrix of asset returns\n",
    "    :type cov_matrix: pd.DataFrame\n",
    "    :param gamma: L2 regularisation parameter, defaults to 0. Increase if you want more\n",
    "                  non-negligible weights\n",
    "    :type gamma: float, optional\n",
    "    :return: portfolio variance\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    L2_reg = gamma * (weights ** 2).sum()\n",
    "    portfolio_volatility = np.dot(weights.T, np.dot(cov_matrix, weights))\n",
    "    return portfolio_volatility + L2_reg\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "def negative_cvar(weights, da,t, s=10000, beta=0.95, random_state=None):\n",
    "    \n",
    "    \n",
    "    if(t==1):\n",
    "        data= da.loc[:,:\"H20\"]\n",
    "        data = data.assign(e=estimated_reward(t))\n",
    "        #print(dataset_reward)\n",
    "        #print(returns)\n",
    "    else:\n",
    "        data = da.loc[:,:\"R\"+str(t-1)]\n",
    "        data = data.assign(e=estimated_reward(t))\n",
    "        \n",
    "    data = data.transpose()\n",
    "    returns = data.pct_change().dropna(how=\"all\")\n",
    "    \n",
    "    #returns = returns.loc[:,:\"R\"+str(t)]\n",
    "    #dataset_reward=dataset_rewards.loc[:,:\"R\"+str(t)]\n",
    "    #returns = returns.pct_change(axis='columns').dropna(how=\"all\")\n",
    "    \n",
    "    #weights=x[i][t]/sum(x[i][t])\n",
    "    np.random.seed(seed=random_state)\n",
    "    # Calcualte the returns given the weights\n",
    "    weights_new = weights.reshape((df.shape[0], 1))\n",
    "    portfolio_returns = ( returns.dot(weights_new)).sum(axis=1)\n",
    "    # Sample from the historical distribution\n",
    "    dist = scipy.stats.gaussian_kde(portfolio_returns)\n",
    "    sample = dist.resample(s)\n",
    "    # Calculate the value at risk\n",
    "    var = portfolio_returns.quantile(1 - beta)\n",
    "    # Mean of all losses worse than the value at risk\n",
    "    return -sample[sample < var].mean()\n",
    "\n",
    "def negative_cvars(weights, da,t, s=10000, beta=0.95, random_state=None):\n",
    "\n",
    "    #d = d.transpose()\n",
    "    returns = da.pct_change().dropna(how=\"all\")\n",
    "    \n",
    "    #returns = returns.loc[:,:\"R\"+str(t)]\n",
    "    #dataset_reward=dataset_rewards.loc[:,:\"R\"+str(t)]\n",
    "    #returns = returns.pct_change(axis='columns').dropna(how=\"all\")\n",
    "    \n",
    "    #weights=x[i][t]/sum(x[i][t])\n",
    "    np.random.seed(seed=random_state)\n",
    "    # Calcualte the returns given the weights\n",
    "    weights_new = weights.reshape((df.shape[0], 1))\n",
    "    portfolio_returns = ( returns.dot(weights_new)).sum(axis=1)\n",
    "    # Sample from the historical distribution\n",
    "    dist = scipy.stats.gaussian_kde(portfolio_returns)\n",
    "    sample = dist.resample(s)\n",
    "    # Calculate the value at risk\n",
    "    var = portfolio_returns.quantile(1 - beta)\n",
    "    # Mean of all losses worse than the value at risk\n",
    "    return -sample[sample < var].mean()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def negative_cvar_plus(weights, da,t, s=10000, beta=0.95, random_state=None):\n",
    "    \"\"\"\n",
    "    Calculate the negative CVaR. Though we want the \"min CVaR portfolio\", we\n",
    "    actually need to maximise the expected return of the worst q% cases, thus\n",
    "    we need this value to be negative.\n",
    "    :param weights: asset weights of the portfolio\n",
    "    :type weights: np.ndarray\n",
    "    :param returns: asset returns\n",
    "    :type returns: pd.DataFrame or np.ndarray\n",
    "    :param s: number of bootstrap draws, defaults to 10000\n",
    "    :type s: int, optional\n",
    "    :param beta: \"significance level\" (i. 1 - q), defaults to 0.95\n",
    "    :type beta: float, optional\n",
    "    :param random_state: seed for random sampling, defaults to None\n",
    "    :type random_state: int, optional\n",
    "    :return: negative CVaR\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    \n",
    "    if(t==1):\n",
    "        data= da.loc[:,:\"H20\"]\n",
    "        data = data.assign(e=estimated_reward_plus(t))\n",
    "        #returns = d.pct_change(axis='columns').dropna(how=\"all\")\n",
    "        #print(dataset_reward)\n",
    "        #print(returns)\n",
    "    else:\n",
    "        data = da.loc[:,:\"R\"+str(t-1)]\n",
    "        data = data.assign(e=estimated_reward_plus(t))\n",
    "        #returns = d.pct_change(axis='columns').dropna(how=\"all\")\n",
    "        \n",
    "    data = data.transpose()\n",
    "    returns = data.pct_change().dropna(how=\"all\")\n",
    "    #weights=x[i][t]/sum(x[i][t])\n",
    "    np.random.seed(seed=random_state)\n",
    "    # Calcualte the returns given the weights\n",
    "    weights_new = weights.reshape((df.shape[0], 1))\n",
    "    portfolio_returns = ( returns.dot(weights_new)).sum(axis=1)\n",
    "    # Sample from the historical distribution\n",
    "    dist = scipy.stats.gaussian_kde(portfolio_returns)\n",
    "    sample = dist.resample(s)\n",
    "    # Calculate the value at risk\n",
    "    var = portfolio_returns.quantile(1 - beta)\n",
    "    # Mean of all losses worse than the value at risk\n",
    "    return -sample[sample < var].mean()\n",
    "\n",
    "def negative_cvar_minus(weights, da,t, s=10000, beta=0.95, random_state=None):\n",
    "    \"\"\"\n",
    "    Calculate the negative CVaR. Though we want the \"min CVaR portfolio\", we\n",
    "    actually need to maximise the expected return of the worst q% cases, thus\n",
    "    we need this value to be negative.\n",
    "    :param weights: asset weights of the portfolio\n",
    "    :type weights: np.ndarray\n",
    "    :param returns: asset returns\n",
    "    :type returns: pd.DataFrame or np.ndarray\n",
    "    :param s: number of bootstrap draws, defaults to 10000\n",
    "    :type s: int, optional\n",
    "    :param beta: \"significance level\" (i. 1 - q), defaults to 0.95\n",
    "    :type beta: float, optional\n",
    "    :param random_state: seed for random sampling, defaults to None\n",
    "    :type random_state: int, optional\n",
    "    :return: negative CVaR\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    if(t==1):\n",
    "        data= da.loc[:,:\"H20\"]\n",
    "        data = data.assign(e=estimated_reward_minus(t))\n",
    "        #returns = d.pct_change(axis='columns').dropna(how=\"all\")\n",
    "        #print(dataset_reward)\n",
    "        #print(returns)\n",
    "    else:\n",
    "        data = da.loc[:,:\"R\"+str(t-1)]\n",
    "        data = data.assign(e=estimated_reward_minus(t))\n",
    "        #returns = d.pct_change(axis='columns').dropna(how=\"all\")\n",
    "        \n",
    "    data = data.transpose()\n",
    "    returns = data.pct_change().dropna(how=\"all\")\n",
    "    \n",
    "    #weights=x[i][t]/sum(x[i][t])\n",
    "    np.random.seed(seed=random_state)\n",
    "    # Calcualte the returns given the weights\n",
    "    weights_new = weights.reshape((df.shape[0], 1))\n",
    "    portfolio_returns = ( returns.dot(weights_new)).sum(axis=1)\n",
    "    # Sample from the historical distribution\n",
    "    dist = scipy.stats.gaussian_kde(portfolio_returns)\n",
    "    sample = dist.resample(s)\n",
    "    # Calculate the value at risk\n",
    "    var = portfolio_returns.quantile(1 - beta)\n",
    "    # Mean of all losses worse than the value at risk\n",
    "    return -sample[sample < var].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The ``value_at_risk`` module allows for optimisation with a (conditional)\n",
    "value-at-risk (CVaR) objective, which requires Monte Carlo simulation.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "#from .base_optimizer import BaseOptimizer\n",
    "#from . import objective_functions   \n",
    "import pandas as pd\n",
    "import noisyopt\n",
    "\n",
    "\n",
    "class CVAROpt(BaseScipyOptimizer):\n",
    "\n",
    "    \"\"\"\n",
    "    A CVAROpt object (inheriting from BaseScipyOptimizer) provides a method for\n",
    "    optimising the CVaR (a.k.a expected shortfall) of a portfolio.\n",
    "    Instance variables:\n",
    "    - Inputs\n",
    "        - ``tickers``\n",
    "        - ``returns``\n",
    "        - ``bounds``\n",
    "    - Optimisation parameters:\n",
    "        - ``s``: the number of Monte Carlo simulations\n",
    "        - ``beta``: the critical value\n",
    "    - Output: ``weights``\n",
    "    Public methods:\n",
    "    - ``min_cvar()``\n",
    "    - ``normalize_weights()``\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, returns, weight_bounds=(0, 1)):\n",
    "        \"\"\"\n",
    "        :param returns: asset historical returns\n",
    "        :type returns: pd.DataFrame\n",
    "        :param weight_bounds: minimum and maximum weight of an asset, defaults to (0, 1).\n",
    "                              Must be changed to (-1, 1) for portfolios with shorting.\n",
    "                              For CVaR opt, this is not a hard boundary.\n",
    "        :type weight_bounds: tuple, optional\n",
    "        :raises TypeError: if ``returns`` is not a dataframe\n",
    "        \"\"\"\n",
    "        if(t==1):\n",
    "            returns= returns.loc[:,:\"H20\"]\n",
    "            returns = returns.assign(e=estimated_reward(t))\n",
    "       \n",
    "        else:\n",
    "            returns = returns.loc[:,:\"R\"+str(t-1)]\n",
    "            returns = returns.assign(e=estimated_reward(t))\n",
    "        \n",
    "        #returns=returns.assign(e=estimated_reward(t))\n",
    "        returns=returns.transpose()\n",
    "        if not isinstance(returns, pd.DataFrame):\n",
    "            raise TypeError(\"returns are not a dataframe\")\n",
    "        self.returns = returns\n",
    "        tickers = returns.columns\n",
    "        super().__init__(len(tickers), tickers, weight_bounds)\n",
    "\n",
    "    def min_cvar(self, t,s=10000, beta=0.95, random_state=None):\n",
    "        \"\"\"\n",
    "        Find the portfolio weights that minimises the CVaR, via\n",
    "        Monte Carlo sampling from the return distribution.\n",
    "        :param s: number of bootstrap draws, defaults to 10000\n",
    "        :type s: int, optional\n",
    "        :param beta: \"significance level\" (i. 1 - q), defaults to 0.95\n",
    "        :type beta: float, optional\n",
    "        :param random_state: seed for random sampling, defaults to None\n",
    "        :type random_state: int, optional\n",
    "        :return: asset weights for the Sharpe-maximising portfolio\n",
    "        :rtype: dict\n",
    "        \"\"\"\n",
    "        args = (self.returns,t, s, beta, random_state)\n",
    "        result = noisyopt.minimizeSPSA(\n",
    "            negative_cvars,\n",
    "            args=args,\n",
    "            bounds=self.bounds,\n",
    "            x0=self.initial_guess,\n",
    "            niter=1000,\n",
    "            paired=False,\n",
    "        )\n",
    "        self.weights = self.normalize_weights(result[\"x\"])\n",
    "        return dict(zip(self.tickers, self.weights))\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_weights(raw_weights):\n",
    "        \"\"\"\n",
    "        Make all weights sum to 1\n",
    "        :param raw_weights: input weights which do not sum to 1\n",
    "        :type raw_weights: np.array, pd.Series\n",
    "        :return: normalized weights\n",
    "        :rtype: np.array, pd.Series\n",
    "        \"\"\"\n",
    "        return raw_weights / raw_weights.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.954237384324752\n",
      "27.74481553853482\n",
      "27.65840151562242\n"
     ]
    }
   ],
   "source": [
    "#cvar if I weight all assets equally \n",
    "#returns = dataset_reward.pct_change(axis='columns').dropna(how=\"all\")\n",
    "#returns = returns.loc[:,:\"R\"+str(t)]\n",
    "w = np.array([1 / df.shape[0]] * df.shape[0])\n",
    "w_new = w.reshape((df.shape[0], 1))\n",
    "d=dataset_reward\n",
    "cvar0plus=negative_cvar_plus(w_new, d,1,s=5000, random_state=0)\n",
    "d=dataset_reward\n",
    "cvar0minus=negative_cvar_minus(w_new, d,1,s=5000, random_state=0)\n",
    "d=dataset_reward\n",
    "cvar0 = negative_cvar(w_new, d,1,s=5000, random_state=0)\n",
    "assert cvar0 > 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#cvar1 = negative_cvar(w_new, d,1, s=5000, beta=0.98, random_state=0)\n",
    "#assert cvar1 > 0\n",
    "\n",
    "#Nondeterministic\n",
    "#cvar2 = negative_cvar(w_new, dataset_reward,1, s=5000, random_state=1)\n",
    "#assert not cvar0 == cvar2\n",
    "print(cvar0minus)\n",
    "print(cvar0)\n",
    "print(cvar0plus)\n",
    "\n",
    "#print(cvar2)\n",
    "#returns \n",
    "#dataset_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7709587090100325\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.04545455, 0.        , 0.04545455, 0.04545455,\n",
       "       0.04545455, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.04545455, 0.04545455, 0.        , 0.04545455,\n",
       "       0.04545455, 0.        , 0.04545455, 0.        , 0.04545455,\n",
       "       0.04545455, 0.04545455, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.04545455, 0.        , 0.04545455, 0.        ,\n",
       "       0.        , 0.        , 0.04545455, 0.04545455, 0.04545455,\n",
       "       0.04545455, 0.04545455, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.04545455, 0.        , 0.04545455, 0.        ,\n",
       "       0.04545455, 0.        ])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#min cvar \n",
    "vr = CVAROpt(dataset_reward)\n",
    "w1 = vr.min_cvar(1,s=5000, random_state=0)\n",
    "assert isinstance(w1, dict)\n",
    "assert set(w1.keys()) == set(dataset_reward.index)\n",
    "assert set(w1.keys()) == set(vr.tickers)\n",
    "np.testing.assert_almost_equal(vr.weights.sum(), 1)\n",
    "\n",
    "#w1\n",
    "w1=np.array(list(dict.values(w1)))\n",
    "#w1\n",
    "w1_new = w1.reshape((df.shape[0], 1))\n",
    "cvar_min = negative_cvar(w1_new, dataset_reward, 1,s=5000, random_state=0)\n",
    "print(cvar_min)\n",
    "#w1_new\n",
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CVaR_minus</th>\n",
       "      <th>CVaR</th>\n",
       "      <th>CVaR_plus</th>\n",
       "      <th>CVaR_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27.954237</td>\n",
       "      <td>27.744816</td>\n",
       "      <td>27.658402</td>\n",
       "      <td>2.287736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.115073</td>\n",
       "      <td>20.123178</td>\n",
       "      <td>20.151719</td>\n",
       "      <td>2.978080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32.375110</td>\n",
       "      <td>23.496402</td>\n",
       "      <td>19.327530</td>\n",
       "      <td>12.575560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44.026012</td>\n",
       "      <td>44.026499</td>\n",
       "      <td>43.965807</td>\n",
       "      <td>7.969598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42.558278</td>\n",
       "      <td>42.558660</td>\n",
       "      <td>42.559085</td>\n",
       "      <td>11.150683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>40.768569</td>\n",
       "      <td>40.827238</td>\n",
       "      <td>40.772650</td>\n",
       "      <td>11.243871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>40.051580</td>\n",
       "      <td>40.052150</td>\n",
       "      <td>39.940427</td>\n",
       "      <td>28.491177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>38.739618</td>\n",
       "      <td>38.739498</td>\n",
       "      <td>38.739379</td>\n",
       "      <td>3.372201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>36.982884</td>\n",
       "      <td>37.058955</td>\n",
       "      <td>37.087603</td>\n",
       "      <td>18.120407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CVaR_minus       CVaR  CVaR_plus   CVaR_min\n",
       "0   27.954237  27.744816  27.658402   2.287736\n",
       "1   20.115073  20.123178  20.151719   2.978080\n",
       "2   32.375110  23.496402  19.327530  12.575560\n",
       "3   44.026012  44.026499  43.965807   7.969598\n",
       "4   42.558278  42.558660  42.559085  11.150683\n",
       "5   40.768569  40.827238  40.772650  11.243871\n",
       "6   40.051580  40.052150  39.940427  28.491177\n",
       "7   38.739618  38.739498  38.739379   3.372201\n",
       "8   36.982884  37.058955  37.087603  18.120407"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#plot CVaR^-,CVaR^+,CVaR,minCVaR with respet to time \n",
    "import matplotlib.pyplot as plt\n",
    "plot_data=pd.DataFrame({})\n",
    "CVaR_minus=[]\n",
    "CVaR_plus=[]\n",
    "CVaR=[]\n",
    "CVaR_min=[]\n",
    "for t in range(1,10):\n",
    "    CVaR_minus.append(negative_cvar_minus(w_new, d,t,s=5000, random_state=0))\n",
    "    CVaR_plus.append(negative_cvar_plus(w_new, d,t,s=5000, random_state=0))\n",
    "    CVaR.append(negative_cvar(w_new, d,t,s=5000, random_state=0))\n",
    "    vr = CVAROpt(dataset_reward)\n",
    "    w1 = vr.min_cvar(t,s=5000, random_state=0)\n",
    "    w1=np.array(list(dict.values(w1)))\n",
    "    w1_new = w1.reshape((df.shape[0], 1))\n",
    "    CVaR_min.append(negative_cvar(w1_new, dataset_reward, t,s=5000, random_state=0))\n",
    "plot_data['CVaR_minus']=CVaR_minus\n",
    "plot_data['CVaR']=CVaR\n",
    "plot_data['CVaR_plus']=CVaR_plus\n",
    "plot_data['CVaR_min']=CVaR_min\n",
    "plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x24fbbcb6ac8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get x_{i,1}'s for all i \\in k \n",
    "def get_start_point(t, weight):\n",
    "    B_divide_c_i=[Cost_Bound/get_cost_for_k(t)[i] for i in range(0,len(k))  ]\n",
    "    #s=sum(x)\n",
    "    #l = [x * 2 for x in l]\n",
    "    x=[weight*B_divide_c_i]\n",
    "    return list(x)\n",
    "    #x=np.array([Cost_Bound / get_cost_for_k(t)] )\n",
    "#get_start_point(1,w1_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test to convert it into array to simple type to be accepted in LP programe \n",
    "#o=get_start_point(1,w1)\n",
    "#o.append(0)\n",
    "#o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Karmarskar algorithm to solve system of linear equation and get x_{i,1} for 1st 2 equations in optimization problem \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class LPSolution(object):\n",
    "    def __init__(self):\n",
    "        self.iterations = None\n",
    "        self.tolerance = None\n",
    "        self.intermediates = []\n",
    "        self.solution = []\n",
    "        self.solution_string = None\n",
    "\n",
    "    def __str__(self):\n",
    "        self.solution_string =str(self.solution)\n",
    "        #self.solution_string += '\\n\\tTolerance: ' + str(self.tolerance)\n",
    "        #self.solution_string += '\\n\\tIterations: ' + str(self.iterations)\n",
    "        return self.solution_string\n",
    "\n",
    "\n",
    "class LinearProgram(object):\n",
    "    \"\"\"A class that implements Karmarkar's Algorithm for the solution of\n",
    "    Linear Programs in standard form.\"\"\"\n",
    "    def __init__(self, A, b, c):\n",
    "        \"\"\"Constructs an n-variable m-constraint Linear Program.\n",
    "\n",
    "        A -- An n x m numpy matrix of constraint coefficients\n",
    "        b -- A 1 x m numpy row vector of constraint RHS values\n",
    "        c -- A 1 x n numpy row vector of objective function coefficients\n",
    "        \"\"\"\n",
    "        self.A = A\n",
    "        self.n, self.m = self.A.shape\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "        self.solution = []\n",
    "\n",
    "    def karmarkar(self, start_point):\n",
    "        \"\"\"Runs one iteration of Karmarkar's Algorithm.\n",
    "\n",
    "        start_point -- A 1 x n numpy row vector of decision variable values\n",
    "        \"\"\"\n",
    "        D = np.diagflat(start_point)\n",
    "        c_tilde = np.matmul(self.c, D)\n",
    "        A_tilde = np.matmul(self.A, D)\n",
    "        A_tildeT = A_tilde.transpose()\n",
    "        AAT_inverse = np.linalg.inv(np.matmul(A_tilde, A_tildeT))\n",
    "        # matrix multiplication is associative\n",
    "        P = np.identity(self.m) - np.matmul(np.matmul(A_tildeT, AAT_inverse), A_tilde)\n",
    "        cp_tilde = np.matmul(c_tilde, P)\n",
    "        k = -0.5 / np.amin(cp_tilde)\n",
    "        x_tilde_new = np.ones((1, self.m), order='F') + k * cp_tilde\n",
    "        return np.matmul(x_tilde_new, D)\n",
    "\n",
    "    def solve(self, start_point, tolerance=1e-5, max_iterations=50, verbose=False):\n",
    "        \"\"\"Uses Karmarkar's Algorithm to solve a Linear Program.\n",
    "\n",
    "        start_point     -- A starting point for Karmarkar's Algorithm. Must be a row vector.\n",
    "        tolerance       -- The stopping tolerance of Karmarkar's Algorithm.\n",
    "        max_iterations  -- The maximum number of iterations to run Karmarkar's Algorithm.\n",
    "        verbose         -- List all intermediate values.\n",
    "        \"\"\"\n",
    "        x = start_point\n",
    "        solution = LPSolution()\n",
    "        for i in range(max_iterations):\n",
    "            x_new = self.karmarkar(x)\n",
    "            if verbose:\n",
    "                print(x_new)\n",
    "\n",
    "            dist = np.linalg.norm(x - x_new)\n",
    "            x = x_new\n",
    "            solution.intermediates.append(x)\n",
    "            if dist < tolerance:\n",
    "                break\n",
    "\n",
    "        solution.solution = x\n",
    "        solution.iterations = i\n",
    "        solution.tolerance = dist\n",
    "        self.solution = solution\n",
    "\n",
    "        return solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#k=get_cost(1)\n",
    "#k.append(1)\n",
    "#!/usr/bin/python3\n",
    "import numpy as np\n",
    "#from linear_program import LinearProgram, LPSolution\n",
    "\n",
    "\n",
    "def generate_tikz_plot(solution):\n",
    "    \"\"\"Generates a 3D Tikz LaTeX coordinate strings for the intermediate solutions.\"\"\"\n",
    "    for i, soln in enumerate(solution.intermediates):\n",
    "        coordinate = r'\\coordinate (xnew{}) at '.format(i)\n",
    "        coordinate += r'({}, {}, {});'.format(*[coord for coord in soln.flat])\n",
    "        print(coordinate)\n",
    "\n",
    "    draw = r'\\draw[red] (x) node[circle, fill, inner sep=1pt] '\n",
    "    for i in range(len(solution.intermediates)):\n",
    "        draw += r'-- (xnew{}) node[circle, fill, inner sep=1pt] '.format(i)\n",
    "    draw += r';'\n",
    "    print(draw)\n",
    "\n",
    "\n",
    "def main(t,start_point):\n",
    "    n=get_cost_for_k(t)\n",
    "    n.append(1)\n",
    "    \n",
    "    m=estimated_reward(t)\n",
    "    m.append(0)\n",
    "    \n",
    "    start_point=start_point.append(0)\n",
    "\n",
    "    \n",
    "    #k.append(1)\n",
    "    #A = np.matrix([get_cost(t), ])\n",
    "    A = np.matrix([n, ])\n",
    "    b = np.array([Cost_Bound, ])\n",
    "    #c = np.array([1, 2, 0])\n",
    "    c = np.array(m)\n",
    "\n",
    "    LP = LinearProgram(A, b, c)\n",
    "    LP.solve(start_point=np.array(start_point))\n",
    "    return (LP.solution)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(1,get_start_point(1,w1))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Convert(string): \n",
    "    li = list(string.split( )) \n",
    "    return li \n",
    "\n",
    "def get_solution_karmarskar_algo(t,start_point):\n",
    "    \n",
    "    y=Convert(str(main(t,start_point)))\n",
    "    x_k_1 = list(filter(None,y )) # fastest\n",
    "    #print(y)\n",
    "    #print(x_k_1)\n",
    "\n",
    "    #removing \"\\n\" part from the list\n",
    "    x_k_1=list(map(str.strip,x_k_1))\n",
    "    #print(x_k_1)\n",
    "\n",
    "    #removing last element from the list\n",
    "    x_k_1=x_k_1[:-1]\n",
    "    #x_k_1\n",
    "\n",
    "    #removing unneccesory elements from 1st element of x_k_1\n",
    "    #print(x_k_1[0])\n",
    "    type(x_k_1)\n",
    "    #sum(float(i) for i in x_k_1)\n",
    "\n",
    "    #print(x_k_1[0])\n",
    "    x_k_1[0]=x_k_1[0][2:]\n",
    "    \n",
    "    #converting every element of x_k_1 which was initially string type to float type\n",
    "    x_k_1=[float(x) for x in x_k_1]\n",
    "    \n",
    "    return x_k_1\n",
    "#get_solution_karmarskar_algo(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y=main()\n",
    "#print(y)\n",
    "#str(y)\n",
    "\n",
    "y=Convert(str(main(1,get_start_point(1,w1))))\n",
    "x_k_1 = list(filter(None,y )) # fastest\n",
    "#print(y)\n",
    "#print(x_k_1)\n",
    "\n",
    "#removing \"\\n\" part from the list\n",
    "x_k_1=list(map(str.strip,x_k_1))\n",
    "#print(x_k_1)\n",
    "\n",
    "#removing last element from the list\n",
    "x_k_1=x_k_1[:-1]\n",
    "#x_k_1\n",
    "\n",
    "#removing unneccesory elements from 1st element of x_k_1\n",
    "#print(x_k_1[0])\n",
    "type(x_k_1)\n",
    "#sum(float(i) for i in x_k_1)\n",
    "\n",
    "#print(x_k_1[0])\n",
    "x_k_1[0]=x_k_1[0][2:]\n",
    "x_k_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting every element of x_k_1 which was initially string type to float type\n",
    "x_k_1=[float(x) for x in x_k_1]\n",
    "x_k_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight_from_securities(x):\n",
    "    weight=[x[i]/sum(x) for i in range(0,len(k))]\n",
    "    return weight\n",
    "weight_from_securities=get_weight_from_securities(x_k_1)\n",
    "weight_from_securities=np.array(w)\n",
    "weight_from_securities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cvar = negative_cvar( weight_from_securities, dataset_reward,1,s=5000, random_state=0)\n",
    "assert cvar > 0\n",
    "cvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def secant(f,a,b,N):\n",
    "    '''Approximate solution of f(x)=0 on interval [a,b] by the secant method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : function\n",
    "        The function for which we are trying to approximate a solution f(x)=0.\n",
    "    a,b : numbers\n",
    "        The interval in which to search for a solution. The function returns\n",
    "        None if f(a)*f(b) >= 0 since a solution is not guaranteed.\n",
    "    N : (positive) integer\n",
    "        The number of iterations to implement.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    m_N : number\n",
    "        The x intercept of the secant line on the the Nth interval\n",
    "            m_n = a_n - f(a_n)*(b_n - a_n)/(f(b_n) - f(a_n))\n",
    "        The initial interval [a_0,b_0] is given by [a,b]. If f(m_n) == 0\n",
    "        for some intercept m_n then the function returns this solution.\n",
    "        If all signs of values f(a_n), f(b_n) and f(m_n) are the same at any\n",
    "        iterations, the secant method fails and return None.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> f = lambda x: x**2 - x - 1\n",
    "    >>> secant(f,1,2,5)\n",
    "    1.6180257510729614\n",
    "    '''\n",
    "    if f(a)*f(b) >= 0:\n",
    "        print(\"Secant method fails.\")\n",
    "        return None\n",
    "    a_n = a\n",
    "    b_n = b\n",
    "    for n in range(1,N+1):\n",
    "        m_n = a_n - f(a_n)*(b_n - a_n)/(f(b_n) - f(a_n))\n",
    "        f_m_n = f(m_n)\n",
    "        if f(a_n)*f_m_n < 0:\n",
    "            a_n = a_n\n",
    "            b_n = m_n\n",
    "        elif f(b_n)*f_m_n < 0:\n",
    "            a_n = m_n\n",
    "            b_n = b_n\n",
    "        elif f_m_n == 0:\n",
    "            print(\"Found exact solution.\")\n",
    "            return m_n\n",
    "        else:\n",
    "            print(\"Secant method fails.\")\n",
    "            return None\n",
    "    return a_n - f(a_n)*(b_n - a_n)/(f(b_n) - f(a_n))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(o):\n",
    "    return (negative_cvar(o, dataset_reward,1,s=5000, random_state=0)-theta*cvar_min)\n",
    "\n",
    "#f=(negative_cvar( w, returns,1,s=5000, random_state=0)-theta*cvar_min)\n",
    "random_guess_for_weight=np.array([1 / df.shape[0]] * df.shape[0])\n",
    "secant(f, weight_from_securities,random_guess_for_weight,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(cvar>theta*cvar_min):\n",
    "    nearby_point=secant(f, weight_from_securities,w1_new,5) \n",
    "    get_solution_karmarskar_algo(1,nearby_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearby_point=secant(f, weight_from_securities,w1_new,5) \n",
    "get_solution_karmarskar_algo(1,nearby_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
